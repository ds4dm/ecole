{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching with Imitation Learning and a GNN\n",
    "\n",
    "In this tutorial we will reproduce a simplified version of the paper of Gasse et al. (2019) on learning to branch with Ecole with `pytorch` and `pytorch geometric`. We collect strong branching examples on randomly generated maximum set covering instances, then train a graph neural network with bipartite state encodings to imitate the expert by classification. Finally, we will evaluate the quality of the policy.\n",
    "\n",
    "The biggest difference with Gasse et al. (2019) is that only n=1,000 training examples of expert decisions are collected for training, to keep the time needed to run the tutorial reasonable. As a consequence, the resulting policy is undertrained and is not competitive with SCIP's default branching rule.\n",
    "\n",
    "Users that are interested in reproducing competitive performance should use a larger sample size, such as the n=100,000 samples used for training in the paper. In this case, we strongly recommend to parallelize data collection, as in the original Gasse et al. (2019) code.\n",
    "\n",
    "### Requirements\n",
    "The requirements can be found in `conda-requirements.yaml`, lock files with pinned versions are also available\n",
    "for various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import ecole\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_MAX_SAMPLES = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "NB_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "Our first step will be to run explore-then-strong-branch on randomly generated maximum set covering instances, and save the branching decisions to build a dataset. We will also record the state of the branch-and-bound process as a bipartite graph, which is already implemented in Ecole with the same features as Gasse et al. (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Ecole-provided set cover instance generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explore-then-strong-branch scheme described in the paper is not implemented by default in Ecole. In this scheme, to diversify the states in which we collect examples of strong branching behavior, we mostly follow a weak but cheap expert (pseudocost branching) and only occasionally call the strong expert (strong branching). This also ensures that samples are closer to being independent and identically distributed.\n",
    "\n",
    "This can be realized in Ecole by creating a custom observation function, which will randomly compute and return the pseudocost scores (cheap) or the strong branching scores (expensive). It also showcases extensibility in Ecole by showing how easily a custom observation function can be created and used, directly in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreThenStrongBranch:\n",
    "    \"\"\"\n",
    "    This custom observation function class will randomly return either strong branching scores (expensive expert)\n",
    "    or pseudocost scores (weak expert for exploration) when called at every node.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expert_probability):\n",
    "        self.expert_probability = expert_probability\n",
    "        self.pseudocosts_function = ecole.observation.Pseudocosts()\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "\n",
    "    def before_reset(self, model):\n",
    "        \"\"\"\n",
    "        This function will be called at initialization of the environment (before dynamics are reset).\n",
    "        \"\"\"\n",
    "        self.pseudocosts_function.before_reset(model)\n",
    "        self.strong_branching_function.before_reset(model)\n",
    "\n",
    "    def extract(self, model, done):\n",
    "        \"\"\"\n",
    "        Should we return strong branching or pseudocost scores at time node?\n",
    "        \"\"\"\n",
    "        probabilities = [1 - self.expert_probability, self.expert_probability]\n",
    "        expert_chosen = bool(np.random.choice(np.arange(2), p=probabilities))\n",
    "        if expert_chosen:\n",
    "            return (self.strong_branching_function.extract(model, done), True)\n",
    "        else:\n",
    "            return (self.pseudocosts_function.extract(model, done), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the environment with the correct parameters (no restarts, 1h time limit, 5% expert sampling probability).\n",
    "\n",
    "Besides the (pseudocost or strong branching) scores, our environment will return the node bipartite graph representation of \n",
    "branch-and-bound states used in Gasse et al. (2019), using the `ecole.observation.NodeBipartite` observation function.\n",
    "On one side of that bipartite graph, nodes represent the variables of the problem, with a vector encoding features of \n",
    "that variable. On the other side of the bipartite graph, nodes represent the constraints of the problem, similarly with \n",
    "a vector encoding features of that constraint. An edge links a variable and a constraint node if the variable participates \n",
    "in that constraint, that is, its coefficient is nonzero in that constraint. The constraint coefficient is attached as an\n",
    "attribute of the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass custom SCIP parameters easily\n",
    "scip_parameters = {\n",
    "    \"separating/maxrounds\": 0,\n",
    "    \"presolving/maxrestarts\": 0,\n",
    "    \"limits/time\": 3600,\n",
    "}\n",
    "\n",
    "# Note how we can tuple observation functions to return complex state information\n",
    "env = ecole.environment.Branching(\n",
    "    observation_function=(\n",
    "        ExploreThenStrongBranch(expert_probability=0.05),\n",
    "        ecole.observation.NodeBipartite(),\n",
    "    ),\n",
    "    scip_params=scip_parameters,\n",
    ")\n",
    "\n",
    "# This will seed the environment for reproducibility\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the instances, following the strong branching expert 5% of the time and saving its decision, until enough samples are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, 2 samples collected so far\n",
      "Episode 2, 3 samples collected so far\n",
      "Episode 3, 4 samples collected so far\n",
      "Episode 4, 4 samples collected so far\n",
      "Episode 5, 8 samples collected so far\n",
      "Episode 6, 8 samples collected so far\n",
      "Episode 7, 10 samples collected so far\n",
      "Episode 8, 10 samples collected so far\n",
      "Episode 9, 10 samples collected so far\n",
      "Episode 10, 11 samples collected so far\n",
      "Episode 11, 11 samples collected so far\n",
      "Episode 12, 88 samples collected so far\n",
      "Episode 13, 88 samples collected so far\n",
      "Episode 14, 91 samples collected so far\n",
      "Episode 15, 91 samples collected so far\n",
      "Episode 16, 91 samples collected so far\n",
      "Episode 17, 91 samples collected so far\n",
      "Episode 18, 92 samples collected so far\n",
      "Episode 19, 93 samples collected so far\n",
      "Episode 20, 99 samples collected so far\n",
      "Episode 21, 103 samples collected so far\n",
      "Episode 22, 106 samples collected so far\n",
      "Episode 23, 106 samples collected so far\n",
      "Episode 24, 107 samples collected so far\n",
      "Episode 25, 117 samples collected so far\n",
      "Episode 26, 171 samples collected so far\n",
      "Episode 27, 171 samples collected so far\n",
      "Episode 28, 172 samples collected so far\n",
      "Episode 29, 174 samples collected so far\n",
      "Episode 30, 175 samples collected so far\n",
      "Episode 31, 175 samples collected so far\n",
      "Episode 32, 175 samples collected so far\n",
      "Episode 33, 175 samples collected so far\n",
      "Episode 34, 175 samples collected so far\n",
      "Episode 35, 182 samples collected so far\n",
      "Episode 36, 183 samples collected so far\n",
      "Episode 37, 194 samples collected so far\n",
      "Episode 38, 194 samples collected so far\n",
      "Episode 39, 195 samples collected so far\n",
      "Episode 40, 196 samples collected so far\n",
      "Episode 41, 196 samples collected so far\n",
      "Episode 42, 198 samples collected so far\n",
      "Episode 43, 203 samples collected so far\n",
      "Episode 44, 205 samples collected so far\n",
      "Episode 45, 205 samples collected so far\n",
      "Episode 46, 205 samples collected so far\n",
      "Episode 47, 205 samples collected so far\n",
      "Episode 48, 207 samples collected so far\n",
      "Episode 49, 208 samples collected so far\n",
      "Episode 50, 208 samples collected so far\n",
      "Episode 51, 208 samples collected so far\n",
      "Episode 52, 208 samples collected so far\n",
      "Episode 53, 208 samples collected so far\n",
      "Episode 54, 208 samples collected so far\n",
      "Episode 55, 208 samples collected so far\n",
      "Episode 56, 222 samples collected so far\n",
      "Episode 57, 228 samples collected so far\n",
      "Episode 58, 239 samples collected so far\n",
      "Episode 59, 239 samples collected so far\n",
      "Episode 60, 240 samples collected so far\n",
      "Episode 61, 240 samples collected so far\n",
      "Episode 62, 242 samples collected so far\n",
      "Episode 63, 242 samples collected so far\n",
      "Episode 64, 242 samples collected so far\n",
      "Episode 65, 248 samples collected so far\n",
      "Episode 66, 248 samples collected so far\n",
      "Episode 67, 249 samples collected so far\n",
      "Episode 68, 249 samples collected so far\n",
      "Episode 69, 249 samples collected so far\n",
      "Episode 70, 250 samples collected so far\n",
      "Episode 71, 251 samples collected so far\n",
      "Episode 72, 252 samples collected so far\n",
      "Episode 73, 254 samples collected so far\n",
      "Episode 74, 255 samples collected so far\n",
      "Episode 75, 256 samples collected so far\n",
      "Episode 76, 256 samples collected so far\n",
      "Episode 77, 283 samples collected so far\n",
      "Episode 78, 287 samples collected so far\n",
      "Episode 79, 288 samples collected so far\n",
      "Episode 80, 288 samples collected so far\n",
      "Episode 81, 348 samples collected so far\n",
      "Episode 82, 354 samples collected so far\n",
      "Episode 83, 354 samples collected so far\n",
      "Episode 84, 354 samples collected so far\n",
      "Episode 85, 355 samples collected so far\n",
      "Episode 86, 363 samples collected so far\n",
      "Episode 87, 363 samples collected so far\n",
      "Episode 88, 363 samples collected so far\n",
      "Episode 89, 363 samples collected so far\n",
      "Episode 90, 363 samples collected so far\n",
      "Episode 91, 365 samples collected so far\n",
      "Episode 92, 366 samples collected so far\n",
      "Episode 93, 367 samples collected so far\n",
      "Episode 94, 367 samples collected so far\n",
      "Episode 95, 368 samples collected so far\n",
      "Episode 96, 374 samples collected so far\n",
      "Episode 97, 374 samples collected so far\n",
      "Episode 98, 374 samples collected so far\n",
      "Episode 99, 374 samples collected so far\n",
      "Episode 100, 375 samples collected so far\n",
      "Episode 101, 375 samples collected so far\n",
      "Episode 102, 378 samples collected so far\n",
      "Episode 103, 380 samples collected so far\n",
      "Episode 104, 383 samples collected so far\n",
      "Episode 105, 384 samples collected so far\n",
      "Episode 106, 393 samples collected so far\n",
      "Episode 107, 394 samples collected so far\n",
      "Episode 108, 395 samples collected so far\n",
      "Episode 109, 396 samples collected so far\n",
      "Episode 110, 406 samples collected so far\n",
      "Episode 111, 415 samples collected so far\n",
      "Episode 112, 419 samples collected so far\n",
      "Episode 113, 419 samples collected so far\n",
      "Episode 114, 420 samples collected so far\n",
      "Episode 115, 421 samples collected so far\n",
      "Episode 116, 421 samples collected so far\n",
      "Episode 117, 421 samples collected so far\n",
      "Episode 118, 423 samples collected so far\n",
      "Episode 119, 423 samples collected so far\n",
      "Episode 120, 425 samples collected so far\n",
      "Episode 121, 426 samples collected so far\n",
      "Episode 122, 426 samples collected so far\n",
      "Episode 123, 426 samples collected so far\n",
      "Episode 124, 429 samples collected so far\n",
      "Episode 125, 436 samples collected so far\n",
      "Episode 126, 444 samples collected so far\n",
      "Episode 127, 445 samples collected so far\n",
      "Episode 128, 448 samples collected so far\n",
      "Episode 129, 450 samples collected so far\n",
      "Episode 130, 451 samples collected so far\n",
      "Episode 131, 469 samples collected so far\n",
      "Episode 132, 469 samples collected so far\n",
      "Episode 133, 476 samples collected so far\n",
      "Episode 134, 481 samples collected so far\n",
      "Episode 135, 483 samples collected so far\n",
      "Episode 136, 486 samples collected so far\n",
      "Episode 137, 487 samples collected so far\n",
      "Episode 138, 488 samples collected so far\n",
      "Episode 139, 489 samples collected so far\n",
      "Episode 140, 489 samples collected so far\n",
      "Episode 141, 502 samples collected so far\n",
      "Episode 142, 518 samples collected so far\n",
      "Episode 143, 518 samples collected so far\n",
      "Episode 144, 520 samples collected so far\n",
      "Episode 145, 522 samples collected so far\n",
      "Episode 146, 522 samples collected so far\n",
      "Episode 147, 522 samples collected so far\n",
      "Episode 148, 522 samples collected so far\n",
      "Episode 149, 523 samples collected so far\n",
      "Episode 150, 523 samples collected so far\n",
      "Episode 151, 523 samples collected so far\n",
      "Episode 152, 523 samples collected so far\n",
      "Episode 153, 524 samples collected so far\n",
      "Episode 154, 527 samples collected so far\n",
      "Episode 155, 530 samples collected so far\n",
      "Episode 156, 530 samples collected so far\n",
      "Episode 157, 530 samples collected so far\n",
      "Episode 158, 531 samples collected so far\n",
      "Episode 159, 539 samples collected so far\n",
      "Episode 160, 539 samples collected so far\n",
      "Episode 161, 542 samples collected so far\n",
      "Episode 162, 543 samples collected so far\n",
      "Episode 163, 543 samples collected so far\n",
      "Episode 164, 554 samples collected so far\n",
      "Episode 165, 556 samples collected so far\n",
      "Episode 166, 558 samples collected so far\n",
      "Episode 167, 564 samples collected so far\n",
      "Episode 168, 565 samples collected so far\n",
      "Episode 169, 565 samples collected so far\n",
      "Episode 170, 565 samples collected so far\n",
      "Episode 171, 577 samples collected so far\n",
      "Episode 172, 577 samples collected so far\n",
      "Episode 173, 578 samples collected so far\n",
      "Episode 174, 578 samples collected so far\n",
      "Episode 175, 581 samples collected so far\n",
      "Episode 176, 583 samples collected so far\n",
      "Episode 177, 583 samples collected so far\n",
      "Episode 178, 584 samples collected so far\n",
      "Episode 179, 584 samples collected so far\n",
      "Episode 180, 585 samples collected so far\n",
      "Episode 181, 585 samples collected so far\n",
      "Episode 182, 585 samples collected so far\n",
      "Episode 183, 586 samples collected so far\n",
      "Episode 184, 606 samples collected so far\n",
      "Episode 185, 608 samples collected so far\n",
      "Episode 186, 609 samples collected so far\n",
      "Episode 187, 610 samples collected so far\n",
      "Episode 188, 610 samples collected so far\n",
      "Episode 189, 610 samples collected so far\n",
      "Episode 190, 611 samples collected so far\n",
      "Episode 191, 611 samples collected so far\n",
      "Episode 192, 613 samples collected so far\n",
      "Episode 193, 614 samples collected so far\n",
      "Episode 194, 616 samples collected so far\n",
      "Episode 195, 617 samples collected so far\n",
      "Episode 196, 621 samples collected so far\n",
      "Episode 197, 621 samples collected so far\n",
      "Episode 198, 624 samples collected so far\n",
      "Episode 199, 629 samples collected so far\n",
      "Episode 200, 629 samples collected so far\n",
      "Episode 201, 629 samples collected so far\n",
      "Episode 202, 629 samples collected so far\n",
      "Episode 203, 631 samples collected so far\n",
      "Episode 204, 631 samples collected so far\n",
      "Episode 205, 632 samples collected so far\n",
      "Episode 206, 639 samples collected so far\n",
      "Episode 207, 640 samples collected so far\n",
      "Episode 208, 642 samples collected so far\n",
      "Episode 209, 642 samples collected so far\n",
      "Episode 210, 650 samples collected so far\n",
      "Episode 211, 650 samples collected so far\n",
      "Episode 212, 653 samples collected so far\n",
      "Episode 213, 655 samples collected so far\n",
      "Episode 214, 655 samples collected so far\n",
      "Episode 215, 655 samples collected so far\n",
      "Episode 216, 655 samples collected so far\n",
      "Episode 217, 656 samples collected so far\n",
      "Episode 218, 657 samples collected so far\n",
      "Episode 219, 657 samples collected so far\n",
      "Episode 220, 660 samples collected so far\n",
      "Episode 221, 662 samples collected so far\n",
      "Episode 222, 663 samples collected so far\n",
      "Episode 223, 663 samples collected so far\n",
      "Episode 224, 667 samples collected so far\n",
      "Episode 225, 667 samples collected so far\n",
      "Episode 226, 668 samples collected so far\n",
      "Episode 227, 670 samples collected so far\n",
      "Episode 228, 670 samples collected so far\n",
      "Episode 229, 671 samples collected so far\n",
      "Episode 230, 679 samples collected so far\n",
      "Episode 231, 679 samples collected so far\n",
      "Episode 232, 679 samples collected so far\n",
      "Episode 233, 680 samples collected so far\n",
      "Episode 234, 687 samples collected so far\n",
      "Episode 235, 689 samples collected so far\n",
      "Episode 236, 691 samples collected so far\n",
      "Episode 237, 699 samples collected so far\n",
      "Episode 238, 700 samples collected so far\n",
      "Episode 239, 700 samples collected so far\n",
      "Episode 240, 700 samples collected so far\n",
      "Episode 241, 700 samples collected so far\n",
      "Episode 242, 707 samples collected so far\n",
      "Episode 243, 708 samples collected so far\n",
      "Episode 244, 711 samples collected so far\n",
      "Episode 245, 712 samples collected so far\n",
      "Episode 246, 712 samples collected so far\n",
      "Episode 247, 712 samples collected so far\n",
      "Episode 248, 722 samples collected so far\n",
      "Episode 249, 722 samples collected so far\n",
      "Episode 250, 729 samples collected so far\n",
      "Episode 251, 729 samples collected so far\n",
      "Episode 252, 729 samples collected so far\n",
      "Episode 253, 734 samples collected so far\n",
      "Episode 254, 738 samples collected so far\n",
      "Episode 255, 739 samples collected so far\n",
      "Episode 256, 741 samples collected so far\n",
      "Episode 257, 741 samples collected so far\n",
      "Episode 258, 741 samples collected so far\n",
      "Episode 259, 741 samples collected so far\n",
      "Episode 260, 741 samples collected so far\n",
      "Episode 261, 741 samples collected so far\n",
      "Episode 262, 741 samples collected so far\n",
      "Episode 263, 743 samples collected so far\n",
      "Episode 264, 743 samples collected so far\n",
      "Episode 265, 744 samples collected so far\n",
      "Episode 266, 749 samples collected so far\n",
      "Episode 267, 751 samples collected so far\n",
      "Episode 268, 753 samples collected so far\n",
      "Episode 269, 753 samples collected so far\n",
      "Episode 270, 753 samples collected so far\n",
      "Episode 271, 754 samples collected so far\n",
      "Episode 272, 754 samples collected so far\n",
      "Episode 273, 756 samples collected so far\n",
      "Episode 274, 756 samples collected so far\n",
      "Episode 275, 756 samples collected so far\n",
      "Episode 276, 756 samples collected so far\n",
      "Episode 277, 757 samples collected so far\n",
      "Episode 278, 757 samples collected so far\n",
      "Episode 279, 759 samples collected so far\n",
      "Episode 280, 759 samples collected so far\n",
      "Episode 281, 760 samples collected so far\n",
      "Episode 282, 763 samples collected so far\n",
      "Episode 283, 771 samples collected so far\n",
      "Episode 284, 772 samples collected so far\n",
      "Episode 285, 772 samples collected so far\n",
      "Episode 286, 772 samples collected so far\n",
      "Episode 287, 773 samples collected so far\n",
      "Episode 288, 774 samples collected so far\n",
      "Episode 289, 781 samples collected so far\n",
      "Episode 290, 784 samples collected so far\n",
      "Episode 291, 805 samples collected so far\n",
      "Episode 292, 805 samples collected so far\n",
      "Episode 293, 806 samples collected so far\n",
      "Episode 294, 813 samples collected so far\n",
      "Episode 295, 815 samples collected so far\n",
      "Episode 296, 816 samples collected so far\n",
      "Episode 297, 818 samples collected so far\n",
      "Episode 298, 818 samples collected so far\n",
      "Episode 299, 818 samples collected so far\n",
      "Episode 300, 818 samples collected so far\n",
      "Episode 301, 818 samples collected so far\n",
      "Episode 302, 819 samples collected so far\n",
      "Episode 303, 821 samples collected so far\n",
      "Episode 304, 822 samples collected so far\n",
      "Episode 305, 822 samples collected so far\n",
      "Episode 306, 822 samples collected so far\n",
      "Episode 307, 825 samples collected so far\n",
      "Episode 308, 825 samples collected so far\n",
      "Episode 309, 825 samples collected so far\n",
      "Episode 310, 826 samples collected so far\n",
      "Episode 311, 829 samples collected so far\n",
      "Episode 312, 831 samples collected so far\n",
      "Episode 313, 831 samples collected so far\n",
      "Episode 314, 833 samples collected so far\n",
      "Episode 315, 836 samples collected so far\n",
      "Episode 316, 836 samples collected so far\n",
      "Episode 317, 837 samples collected so far\n",
      "Episode 318, 837 samples collected so far\n",
      "Episode 319, 837 samples collected so far\n",
      "Episode 320, 837 samples collected so far\n",
      "Episode 321, 837 samples collected so far\n",
      "Episode 322, 841 samples collected so far\n",
      "Episode 323, 865 samples collected so far\n",
      "Episode 324, 877 samples collected so far\n",
      "Episode 325, 881 samples collected so far\n",
      "Episode 326, 881 samples collected so far\n",
      "Episode 327, 881 samples collected so far\n",
      "Episode 328, 882 samples collected so far\n",
      "Episode 329, 882 samples collected so far\n",
      "Episode 330, 882 samples collected so far\n",
      "Episode 331, 886 samples collected so far\n",
      "Episode 332, 893 samples collected so far\n",
      "Episode 333, 897 samples collected so far\n",
      "Episode 334, 897 samples collected so far\n",
      "Episode 335, 897 samples collected so far\n",
      "Episode 336, 901 samples collected so far\n",
      "Episode 337, 906 samples collected so far\n",
      "Episode 338, 907 samples collected so far\n",
      "Episode 339, 907 samples collected so far\n",
      "Episode 340, 907 samples collected so far\n",
      "Episode 341, 907 samples collected so far\n",
      "Episode 342, 910 samples collected so far\n",
      "Episode 343, 910 samples collected so far\n",
      "Episode 344, 910 samples collected so far\n",
      "Episode 345, 911 samples collected so far\n",
      "Episode 346, 911 samples collected so far\n",
      "Episode 347, 913 samples collected so far\n",
      "Episode 348, 917 samples collected so far\n",
      "Episode 349, 917 samples collected so far\n",
      "Episode 350, 917 samples collected so far\n",
      "Episode 351, 919 samples collected so far\n",
      "Episode 352, 919 samples collected so far\n",
      "Episode 353, 923 samples collected so far\n",
      "Episode 354, 927 samples collected so far\n",
      "Episode 355, 928 samples collected so far\n",
      "Episode 356, 928 samples collected so far\n",
      "Episode 357, 933 samples collected so far\n",
      "Episode 358, 938 samples collected so far\n",
      "Episode 359, 944 samples collected so far\n",
      "Episode 360, 946 samples collected so far\n",
      "Episode 361, 949 samples collected so far\n",
      "Episode 362, 953 samples collected so far\n",
      "Episode 363, 954 samples collected so far\n",
      "Episode 364, 957 samples collected so far\n",
      "Episode 365, 960 samples collected so far\n",
      "Episode 366, 962 samples collected so far\n",
      "Episode 367, 968 samples collected so far\n",
      "Episode 368, 968 samples collected so far\n",
      "Episode 369, 968 samples collected so far\n",
      "Episode 370, 975 samples collected so far\n",
      "Episode 371, 975 samples collected so far\n",
      "Episode 372, 975 samples collected so far\n",
      "Episode 373, 976 samples collected so far\n",
      "Episode 374, 977 samples collected so far\n",
      "Episode 375, 979 samples collected so far\n",
      "Episode 376, 979 samples collected so far\n",
      "Episode 377, 979 samples collected so far\n",
      "Episode 378, 980 samples collected so far\n",
      "Episode 379, 980 samples collected so far\n",
      "Episode 380, 980 samples collected so far\n",
      "Episode 381, 980 samples collected so far\n",
      "Episode 382, 980 samples collected so far\n",
      "Episode 383, 981 samples collected so far\n",
      "Episode 384, 983 samples collected so far\n",
      "Episode 385, 991 samples collected so far\n",
      "Episode 386, 998 samples collected so far\n",
      "Episode 387, 998 samples collected so far\n",
      "Episode 388, 1000 samples collected so far\n"
     ]
    }
   ],
   "source": [
    "episode_counter, sample_counter = 0, 0\n",
    "Path(\"samples/\").mkdir(exist_ok=True)\n",
    "\n",
    "# We will solve problems (run episodes) until we have saved enough samples\n",
    "while sample_counter < DATA_MAX_SAMPLES:\n",
    "    episode_counter += 1\n",
    "\n",
    "    observation, action_set, _, done, _ = env.reset(next(instances))\n",
    "    while not done:\n",
    "        (scores, scores_are_expert), node_observation = observation\n",
    "        action = action_set[scores[action_set].argmax()]\n",
    "\n",
    "        # Only save samples if they are coming from the expert (strong branching)\n",
    "        if scores_are_expert and (sample_counter < DATA_MAX_SAMPLES):\n",
    "            sample_counter += 1\n",
    "            data = [node_observation, action, action_set, scores]\n",
    "            filename = f\"samples/sample_{sample_counter}.pkl\"\n",
    "\n",
    "            with gzip.open(filename, \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "\n",
    "        observation, action_set, _, done, _ = env.step(action)\n",
    "\n",
    "    print(f\"Episode {episode_counter}, {sample_counter} samples collected so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a GNN\n",
    "\n",
    "Our next step is to train a GNN classifier on these collected samples to predict similar choices to strong branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define pytorch geometric data classes to handle the bipartite graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    \"\"\"\n",
    "    This class encode a node bipartite graph observation as returned by the `ecole.observation.NodeBipartite`\n",
    "    observation function in a format understood by the pytorch geometric data handlers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        constraint_features,\n",
    "        edge_indices,\n",
    "        edge_features,\n",
    "        variable_features,\n",
    "        candidates,\n",
    "        nb_candidates,\n",
    "        candidate_choice,\n",
    "        candidate_scores,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.constraint_features = constraint_features\n",
    "        self.edge_index = edge_indices\n",
    "        self.edge_attr = edge_features\n",
    "        self.variable_features = variable_features\n",
    "        self.candidates = candidates\n",
    "        self.nb_candidates = nb_candidates\n",
    "        self.candidate_choices = candidate_choice\n",
    "        self.candidate_scores = candidate_scores\n",
    "\n",
    "    def __inc__(self, key, value, store, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        We overload the pytorch geometric method that tells how to increment indices when concatenating graphs\n",
    "        for those entries (edge index, candidates) for which this is not obvious.\n",
    "        \"\"\"\n",
    "        if key == \"edge_index\":\n",
    "            return torch.tensor(\n",
    "                [[self.constraint_features.size(0)], [self.variable_features.size(0)]]\n",
    "            )\n",
    "        elif key == \"candidates\":\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "class GraphDataset(torch_geometric.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class encodes a collection of graphs, as well as a method to load such graphs from the disk.\n",
    "    It can be used in turn by the data loaders provided by pytorch geometric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_files):\n",
    "        super().__init__(root=None, transform=None, pre_transform=None)\n",
    "        self.sample_files = sample_files\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sample_files)\n",
    "\n",
    "    def get(self, index):\n",
    "        \"\"\"\n",
    "        This method loads a node bipartite graph observation as saved on the disk during data collection.\n",
    "        \"\"\"\n",
    "        with gzip.open(self.sample_files[index], \"rb\") as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action, sample_action_set, sample_scores = sample\n",
    "        \n",
    "        constraint_features = sample_observation.row_features\n",
    "        edge_indices = sample_observation.edge_features.indices.astype(np.int32)\n",
    "        edge_features = np.expand_dims(sample_observation.edge_features.values, axis=-1)\n",
    "        variable_features = sample_observation.column_features\n",
    "\n",
    "        # We note on which variables we were allowed to branch, the scores as well as the choice\n",
    "        # taken by strong branching (relative to the candidates)\n",
    "        candidates = np.array(sample_action_set, dtype=np.int32)\n",
    "        candidate_scores = np.array([sample_scores[j] for j in candidates])\n",
    "        candidate_choice = np.where(candidates == sample_action)[0][0]\n",
    "\n",
    "        graph = BipartiteNodeData(\n",
    "            torch.FloatTensor(constraint_features),\n",
    "            torch.LongTensor(edge_indices),\n",
    "            torch.FloatTensor(edge_features),\n",
    "            torch.FloatTensor(variable_features),\n",
    "            torch.LongTensor(candidates),\n",
    "            len(candidates),\n",
    "            torch.LongTensor([candidate_choice]),\n",
    "            torch.FloatTensor(candidate_scores)\n",
    "        )\n",
    "\n",
    "        # We must tell pytorch geometric how many nodes there are, for indexing purposes\n",
    "        graph.num_nodes = constraint_features.shape[0] + variable_features.shape[0]\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then prepare the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = [str(path) for path in Path(\"samples/\").glob(\"sample_*.pkl\")]\n",
    "train_files = sample_files[: int(0.8 * len(sample_files))]\n",
    "valid_files = sample_files[int(0.8 * len(sample_files)) :]\n",
    "\n",
    "train_data = GraphDataset(train_files)\n",
    "train_loader = torch_geometric.loader.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_data = GraphDataset(valid_files)\n",
    "valid_loader = torch_geometric.loader.DataLoader(valid_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our graph neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNPolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        emb_size = 64\n",
    "        cons_nfeats = 5\n",
    "        edge_nfeats = 1\n",
    "        var_nfeats = 19\n",
    "\n",
    "        # CONSTRAINT EMBEDDING\n",
    "        self.cons_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(cons_nfeats),\n",
    "            torch.nn.Linear(cons_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # EDGE EMBEDDING\n",
    "        self.edge_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(edge_nfeats),\n",
    "        )\n",
    "\n",
    "        # VARIABLE EMBEDDING\n",
    "        self.var_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(var_nfeats),\n",
    "            torch.nn.Linear(var_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_v_to_c = BipartiteGraphConvolution()\n",
    "        self.conv_c_to_v = BipartiteGraphConvolution()\n",
    "\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, constraint_features, edge_indices, edge_features, variable_features\n",
    "    ):\n",
    "        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n",
    "\n",
    "        # First step: linear embedding layers to a common dimension (64)\n",
    "        constraint_features = self.cons_embedding(constraint_features)\n",
    "        edge_features = self.edge_embedding(edge_features)\n",
    "        variable_features = self.var_embedding(variable_features)\n",
    "\n",
    "        # Two half convolutions\n",
    "        constraint_features = self.conv_v_to_c(\n",
    "            variable_features, reversed_edge_indices, edge_features, constraint_features\n",
    "        )\n",
    "        variable_features = self.conv_c_to_v(\n",
    "            constraint_features, edge_indices, edge_features, variable_features\n",
    "        )\n",
    "\n",
    "        # A final MLP on the variable features\n",
    "        output = self.output_module(variable_features).squeeze(-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n",
    "    \"\"\"\n",
    "    The bipartite graph convolution is already provided by pytorch geometric and we merely need\n",
    "    to provide the exact form of the messages being passed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"add\")\n",
    "        emb_size = 64\n",
    "\n",
    "        self.feature_module_left = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        self.feature_module_edge = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_right = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_final = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "        self.post_conv_module = torch.nn.Sequential(torch.nn.LayerNorm(emb_size))\n",
    "\n",
    "        # output_layers\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, left_features, edge_indices, edge_features, right_features):\n",
    "        \"\"\"\n",
    "        This method sends the messages, computed in the message method.\n",
    "        \"\"\"\n",
    "        output = self.propagate(\n",
    "            edge_indices,\n",
    "            size=(left_features.shape[0], right_features.shape[0]),\n",
    "            node_features=(left_features, right_features),\n",
    "            edge_features=edge_features,\n",
    "        )\n",
    "        return self.output_module(\n",
    "            torch.cat([self.post_conv_module(output), right_features], dim=-1)\n",
    "        )\n",
    "\n",
    "    def message(self, node_features_i, node_features_j, edge_features):\n",
    "        output = self.feature_module_final(\n",
    "            self.feature_module_left(node_features_i)\n",
    "            + self.feature_module_edge(edge_features)\n",
    "            + self.feature_module_right(node_features_j)\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "policy = GNNPolicy().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we can predict a probability distribution over actions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0155, 0.0153, 0.0153, 0.0154, 0.0153, 0.0153, 0.0154, 0.0154, 0.0153,\n",
      "        0.0154, 0.0155, 0.0153, 0.0154, 0.0153, 0.0154, 0.0154, 0.0153, 0.0153,\n",
      "        0.0154, 0.0154, 0.0154, 0.0153, 0.0154, 0.0154, 0.0154, 0.0153, 0.0153,\n",
      "        0.0154, 0.0154, 0.0153, 0.0153, 0.0154, 0.0154, 0.0155, 0.0153, 0.0154,\n",
      "        0.0153, 0.0153, 0.0154, 0.0154, 0.0153, 0.0154, 0.0154, 0.0154, 0.0154,\n",
      "        0.0154, 0.0153, 0.0153, 0.0154, 0.0154, 0.0154, 0.0155, 0.0154, 0.0154,\n",
      "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
      "        0.0154, 0.0154], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "observation = train_data[0].to(DEVICE)\n",
    "\n",
    "logits = policy(\n",
    "    observation.constraint_features,\n",
    "    observation.edge_index,\n",
    "    observation.edge_attr,\n",
    "    observation.variable_features,\n",
    ")\n",
    "action_distribution = F.softmax(logits[observation.candidates], dim=-1)\n",
    "\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, with randomly initialized weights, the initial distributions tend to be close to uniform.\n",
    "Next, we will define two helper functions: one to train or evaluate the model on a whole epoch and compute metrics for monitoring, and one for padding tensors when doing predictions on a batch of graphs of potentially different number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(policy, data_loader, optimizer=None):\n",
    "    \"\"\"\n",
    "    This function will process a whole epoch of training or validation, depending on whether an optimizer is provided.\n",
    "    \"\"\"\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            # Compute the logits (i.e. pre-softmax activations) according to the policy on the concatenated graphs\n",
    "            logits = policy(\n",
    "                batch.constraint_features,\n",
    "                batch.edge_index,\n",
    "                batch.edge_attr,\n",
    "                batch.variable_features,\n",
    "            )\n",
    "            # Index the results by the candidates, and split and pad them\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "            # Compute the usual cross-entropy classification loss\n",
    "            loss = F.cross_entropy(logits, batch.candidate_choices)\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            true_scores = pad_tensor(batch.candidate_scores, batch.nb_candidates)\n",
    "            true_bestscore = true_scores.max(dim=-1, keepdims=True).values\n",
    "\n",
    "            predicted_bestindex = logits.max(dim=-1, keepdims=True).indices\n",
    "            accuracy = (\n",
    "                (true_scores.gather(-1, predicted_bestindex) == true_bestscore)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            mean_loss += loss.item() * batch.num_graphs\n",
    "            mean_acc += accuracy * batch.num_graphs\n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_acc /= n_samples_processed\n",
    "    return mean_loss, mean_acc\n",
    "\n",
    "\n",
    "def pad_tensor(input_, pad_sizes, pad_value=-1e8):\n",
    "    \"\"\"\n",
    "    This utility function splits a tensor and pads each split to make them all the same size, then stacks them.\n",
    "    \"\"\"\n",
    "    max_pad_size = pad_sizes.max()\n",
    "    output = input_.split(pad_sizes.cpu().numpy().tolist())\n",
    "    output = torch.stack(\n",
    "        [\n",
    "            F.pad(slice_, (0, max_pad_size - slice_.size(0)), \"constant\", pad_value)\n",
    "            for slice_ in output\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can actually create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 4.116, accuracy 0.294\n",
      "Valid loss: 3.767, accuracy 0.305\n",
      "Epoch 2\n",
      "Train loss: 3.720, accuracy 0.399\n",
      "Valid loss: 3.390, accuracy 0.440\n",
      "Epoch 3\n",
      "Train loss: 3.485, accuracy 0.465\n",
      "Valid loss: 3.275, accuracy 0.430\n",
      "Epoch 4\n",
      "Train loss: 3.490, accuracy 0.469\n",
      "Valid loss: 3.289, accuracy 0.385\n",
      "Epoch 5\n",
      "Train loss: 3.489, accuracy 0.484\n",
      "Valid loss: 3.275, accuracy 0.435\n",
      "Epoch 6\n",
      "Train loss: 3.447, accuracy 0.505\n",
      "Valid loss: 3.241, accuracy 0.445\n",
      "Epoch 7\n",
      "Train loss: 3.427, accuracy 0.501\n",
      "Valid loss: 3.239, accuracy 0.430\n",
      "Epoch 8\n",
      "Train loss: 3.441, accuracy 0.500\n",
      "Valid loss: 3.222, accuracy 0.405\n",
      "Epoch 9\n",
      "Train loss: 3.424, accuracy 0.502\n",
      "Valid loss: 3.227, accuracy 0.465\n",
      "Epoch 10\n",
      "Train loss: 3.420, accuracy 0.491\n",
      "Valid loss: 3.283, accuracy 0.410\n",
      "Epoch 11\n",
      "Train loss: 3.418, accuracy 0.486\n",
      "Valid loss: 3.206, accuracy 0.440\n",
      "Epoch 12\n",
      "Train loss: 3.399, accuracy 0.497\n",
      "Valid loss: 3.174, accuracy 0.455\n",
      "Epoch 13\n",
      "Train loss: 3.383, accuracy 0.506\n",
      "Valid loss: 3.245, accuracy 0.440\n",
      "Epoch 14\n",
      "Train loss: 3.391, accuracy 0.512\n",
      "Valid loss: 3.182, accuracy 0.435\n",
      "Epoch 15\n",
      "Train loss: 3.370, accuracy 0.517\n",
      "Valid loss: 3.183, accuracy 0.460\n",
      "Epoch 16\n",
      "Train loss: 3.381, accuracy 0.507\n",
      "Valid loss: 3.206, accuracy 0.425\n",
      "Epoch 17\n",
      "Train loss: 3.336, accuracy 0.520\n",
      "Valid loss: 3.149, accuracy 0.455\n",
      "Epoch 18\n",
      "Train loss: 3.342, accuracy 0.517\n",
      "Valid loss: 3.183, accuracy 0.450\n",
      "Epoch 19\n",
      "Train loss: 3.363, accuracy 0.525\n",
      "Valid loss: 3.144, accuracy 0.450\n",
      "Epoch 20\n",
      "Train loss: 3.322, accuracy 0.514\n",
      "Valid loss: 3.182, accuracy 0.455\n",
      "Epoch 21\n",
      "Train loss: 3.316, accuracy 0.526\n",
      "Valid loss: 3.131, accuracy 0.475\n",
      "Epoch 22\n",
      "Train loss: 3.283, accuracy 0.522\n",
      "Valid loss: 3.130, accuracy 0.465\n",
      "Epoch 23\n",
      "Train loss: 3.232, accuracy 0.552\n",
      "Valid loss: 3.119, accuracy 0.430\n",
      "Epoch 24\n",
      "Train loss: 3.221, accuracy 0.529\n",
      "Valid loss: 3.147, accuracy 0.465\n",
      "Epoch 25\n",
      "Train loss: 3.302, accuracy 0.520\n",
      "Valid loss: 3.244, accuracy 0.435\n",
      "Epoch 26\n",
      "Train loss: 3.248, accuracy 0.511\n",
      "Valid loss: 3.058, accuracy 0.475\n",
      "Epoch 27\n",
      "Train loss: 3.145, accuracy 0.530\n",
      "Valid loss: 3.075, accuracy 0.455\n",
      "Epoch 28\n",
      "Train loss: 3.136, accuracy 0.524\n",
      "Valid loss: 2.994, accuracy 0.450\n",
      "Epoch 29\n",
      "Train loss: 3.123, accuracy 0.530\n",
      "Valid loss: 2.982, accuracy 0.495\n",
      "Epoch 30\n",
      "Train loss: 3.098, accuracy 0.550\n",
      "Valid loss: 3.010, accuracy 0.470\n",
      "Epoch 31\n",
      "Train loss: 3.066, accuracy 0.546\n",
      "Valid loss: 3.024, accuracy 0.485\n",
      "Epoch 32\n",
      "Train loss: 3.088, accuracy 0.527\n",
      "Valid loss: 3.022, accuracy 0.460\n",
      "Epoch 33\n",
      "Train loss: 3.119, accuracy 0.540\n",
      "Valid loss: 3.110, accuracy 0.425\n",
      "Epoch 34\n",
      "Train loss: 3.070, accuracy 0.527\n",
      "Valid loss: 3.070, accuracy 0.440\n",
      "Epoch 35\n",
      "Train loss: 3.054, accuracy 0.531\n",
      "Valid loss: 3.034, accuracy 0.445\n",
      "Epoch 36\n",
      "Train loss: 3.045, accuracy 0.544\n",
      "Valid loss: 3.073, accuracy 0.435\n",
      "Epoch 37\n",
      "Train loss: 3.036, accuracy 0.535\n",
      "Valid loss: 3.001, accuracy 0.470\n",
      "Epoch 38\n",
      "Train loss: 3.052, accuracy 0.539\n",
      "Valid loss: 3.097, accuracy 0.435\n",
      "Epoch 39\n",
      "Train loss: 3.110, accuracy 0.529\n",
      "Valid loss: 2.977, accuracy 0.435\n",
      "Epoch 40\n",
      "Train loss: 3.071, accuracy 0.537\n",
      "Valid loss: 3.034, accuracy 0.450\n",
      "Epoch 41\n",
      "Train loss: 3.064, accuracy 0.545\n",
      "Valid loss: 3.006, accuracy 0.485\n",
      "Epoch 42\n",
      "Train loss: 3.002, accuracy 0.557\n",
      "Valid loss: 3.027, accuracy 0.410\n",
      "Epoch 43\n",
      "Train loss: 3.000, accuracy 0.540\n",
      "Valid loss: 3.044, accuracy 0.430\n",
      "Epoch 44\n",
      "Train loss: 3.005, accuracy 0.531\n",
      "Valid loss: 3.032, accuracy 0.465\n",
      "Epoch 45\n",
      "Train loss: 3.019, accuracy 0.552\n",
      "Valid loss: 3.055, accuracy 0.415\n",
      "Epoch 46\n",
      "Train loss: 2.972, accuracy 0.531\n",
      "Valid loss: 3.039, accuracy 0.440\n",
      "Epoch 47\n",
      "Train loss: 3.006, accuracy 0.536\n",
      "Valid loss: 3.016, accuracy 0.445\n",
      "Epoch 48\n",
      "Train loss: 3.008, accuracy 0.531\n",
      "Valid loss: 3.041, accuracy 0.440\n",
      "Epoch 49\n",
      "Train loss: 3.006, accuracy 0.545\n",
      "Valid loss: 3.002, accuracy 0.450\n",
      "Epoch 50\n",
      "Train loss: 2.946, accuracy 0.556\n",
      "Valid loss: 3.030, accuracy 0.450\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    train_loss, train_acc = process(policy, train_loader, optimizer)\n",
    "    print(f\"Train loss: {train_loss:0.3f}, accuracy {train_acc:0.3f}\")\n",
    "\n",
    "    valid_loss, valid_acc = process(policy, valid_loader, None)\n",
    "    print(f\"Valid loss: {valid_loss:0.3f}, accuracy {valid_acc:0.3f}\")\n",
    "\n",
    "torch.save(policy.state_dict(), \"trained_params.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluation\n",
    "\n",
    "Finally, we can evaluate the performance of the model. We first define appropriate environments. For benchmarking purposes, we include a trivial environment that merely runs SCIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scip_parameters = {\n",
    "    \"separating/maxrounds\": 0,\n",
    "    \"presolving/maxrestarts\": 0,\n",
    "    \"limits/time\": 3600,\n",
    "}\n",
    "env = ecole.environment.Branching(\n",
    "    observation_function=ecole.observation.NodeBipartite(),\n",
    "    information_function={\n",
    "        \"nb_nodes\": ecole.reward.NNodes(),\n",
    "        \"time\": ecole.reward.SolvingTime(),\n",
    "    },\n",
    "    scip_params=scip_parameters,\n",
    ")\n",
    "default_env = ecole.environment.Configuring(\n",
    "    observation_function=None,\n",
    "    information_function={\n",
    "        \"nb_nodes\": ecole.reward.NNodes(),\n",
    "        \"time\": ecole.reward.SolvingTime(),\n",
    "    },\n",
    "    scip_params=scip_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply follow the environments, taking steps appropriately according to the GNN policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes      97  | SCIP time     4.98 \n",
      "             | GNN  nb nodes     466  | GNN  time     4.19 \n",
      "             | Gain          -380.41% | Gain         15.80%\n",
      "Instance   1 | SCIP nb nodes      13  | SCIP time     2.54 \n",
      "             | GNN  nb nodes      85  | GNN  time     1.76 \n",
      "             | Gain          -553.85% | Gain         30.84%\n",
      "Instance   2 | SCIP nb nodes      11  | SCIP time     2.79 \n",
      "             | GNN  nb nodes     154  | GNN  time     2.41 \n",
      "             | Gain         -1300.00% | Gain         13.60%\n",
      "Instance   3 | SCIP nb nodes       1  | SCIP time     1.67 \n",
      "             | GNN  nb nodes      12  | GNN  time     1.19 \n",
      "             | Gain         -1100.00% | Gain         28.74%\n",
      "Instance   4 | SCIP nb nodes      23  | SCIP time     3.73 \n",
      "             | GNN  nb nodes     195  | GNN  time     2.46 \n",
      "             | Gain          -747.83% | Gain         34.12%\n",
      "Instance   5 | SCIP nb nodes       3  | SCIP time     1.92 \n",
      "             | GNN  nb nodes      31  | GNN  time     1.39 \n",
      "             | Gain          -933.33% | Gain         27.76%\n",
      "Instance   6 | SCIP nb nodes       1  | SCIP time     1.30 \n",
      "             | GNN  nb nodes       1  | GNN  time     1.47 \n",
      "             | Gain             0.00% | Gain        -13.20%\n",
      "Instance   7 | SCIP nb nodes       1  | SCIP time     1.32 \n",
      "             | GNN  nb nodes       1  | GNN  time     1.38 \n",
      "             | Gain             0.00% | Gain         -3.89%\n",
      "Instance   8 | SCIP nb nodes       3  | SCIP time     2.31 \n",
      "             | GNN  nb nodes      29  | GNN  time     1.56 \n",
      "             | Gain          -866.67% | Gain         32.55%\n",
      "Instance   9 | SCIP nb nodes       3  | SCIP time     2.04 \n",
      "             | GNN  nb nodes      25  | GNN  time     1.50 \n",
      "             | Gain          -733.33% | Gain         26.25%\n",
      "Instance  10 | SCIP nb nodes       7  | SCIP time     1.59 \n",
      "             | GNN  nb nodes      75  | GNN  time     1.52 \n",
      "             | Gain          -971.43% | Gain          4.56%\n",
      "Instance  11 | SCIP nb nodes      11  | SCIP time     2.76 \n",
      "             | GNN  nb nodes      93  | GNN  time     2.37 \n",
      "             | Gain          -745.45% | Gain         14.02%\n",
      "Instance  12 | SCIP nb nodes      73  | SCIP time     3.58 \n",
      "             | GNN  nb nodes     185  | GNN  time     2.59 \n",
      "             | Gain          -153.42% | Gain         27.68%\n",
      "Instance  13 | SCIP nb nodes       1  | SCIP time     0.03 \n",
      "             | GNN  nb nodes       1  | GNN  time     0.03 \n",
      "             | Gain             0.00% | Gain          1.69%\n",
      "Instance  14 | SCIP nb nodes       3  | SCIP time     1.55 \n",
      "             | GNN  nb nodes      21  | GNN  time     1.38 \n",
      "             | Gain          -600.00% | Gain         11.01%\n",
      "Instance  15 | SCIP nb nodes       7  | SCIP time     2.31 \n",
      "             | GNN  nb nodes      51  | GNN  time     1.65 \n",
      "             | Gain          -628.57% | Gain         28.30%\n",
      "Instance  16 | SCIP nb nodes       3  | SCIP time     1.59 \n",
      "             | GNN  nb nodes      13  | GNN  time     1.47 \n",
      "             | Gain          -333.33% | Gain          7.35%\n",
      "Instance  17 | SCIP nb nodes       1  | SCIP time     1.10 \n",
      "             | GNN  nb nodes       5  | GNN  time     1.16 \n",
      "             | Gain          -400.00% | Gain         -5.15%\n",
      "Instance  18 | SCIP nb nodes      15  | SCIP time     2.18 \n",
      "             | GNN  nb nodes     153  | GNN  time     1.74 \n",
      "             | Gain          -920.00% | Gain         20.00%\n",
      "Instance  19 | SCIP nb nodes      13  | SCIP time     3.26 \n",
      "             | GNN  nb nodes      61  | GNN  time     2.54 \n",
      "             | Gain          -369.23% | Gain         22.27%\n"
     ]
    }
   ],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "for instance_count, instance in zip(range(20), instances):\n",
    "    # Run the GNN brancher\n",
    "    nb_nodes, time = 0, 0\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    nb_nodes += info[\"nb_nodes\"]\n",
    "    time += info[\"time\"]\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (\n",
    "                torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE),\n",
    "                torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE),\n",
    "            )\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "        nb_nodes += info[\"nb_nodes\"]\n",
    "        time += info[\"time\"]\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance)\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "\n",
    "    print(f\"Instance {instance_count: >3} | SCIP nb nodes    {int(default_info['nb_nodes']): >4d}  | SCIP time   {default_info['time']: >6.2f} \")\n",
    "    print(f\"             | GNN  nb nodes    {int(nb_nodes): >4d}  | GNN  time   {time: >6.2f} \")\n",
    "    print(f\"             | Gain         {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}% | Gain      {100*(1-time/default_info['time']): >8.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate on instances larger and harder than those trained on, say with 600 rather than 500 constraints.\n",
    "In addition, we showcase that the cumulative number of nodes and time required to solve an instance can also be computed directly using the `.cumsum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes      29  | SCIP time     3.21 \n",
      "             | GNN  nb nodes     113  | GNN  time     2.12 \n",
      "             | Gain          -289.66% | Gain         34.19%\n",
      "Instance   1 | SCIP nb nodes       9  | SCIP time     3.32 \n",
      "             | GNN  nb nodes      93  | GNN  time     2.57 \n",
      "             | Gain          -933.33% | Gain         22.41%\n",
      "Instance   2 | SCIP nb nodes      17  | SCIP time     4.60 \n",
      "             | GNN  nb nodes     103  | GNN  time     3.13 \n",
      "             | Gain          -505.88% | Gain         31.89%\n",
      "Instance   3 | SCIP nb nodes       3  | SCIP time     2.63 \n",
      "             | GNN  nb nodes      19  | GNN  time     1.89 \n",
      "             | Gain          -533.33% | Gain         28.39%\n",
      "Instance   4 | SCIP nb nodes       7  | SCIP time     2.88 \n",
      "             | GNN  nb nodes      52  | GNN  time     2.33 \n",
      "             | Gain          -642.86% | Gain         19.36%\n",
      "Instance   5 | SCIP nb nodes     201  | SCIP time     5.39 \n",
      "             | GNN  nb nodes     569  | GNN  time     4.66 \n",
      "             | Gain          -183.08% | Gain         13.69%\n",
      "Instance   6 | SCIP nb nodes     912  | SCIP time     9.93 \n",
      "             | GNN  nb nodes    1548  | GNN  time    10.13 \n",
      "             | Gain           -69.74% | Gain         -2.02%\n",
      "Instance   7 | SCIP nb nodes       7  | SCIP time     3.50 \n",
      "             | GNN  nb nodes      75  | GNN  time     2.54 \n",
      "             | Gain          -971.43% | Gain         27.37%\n",
      "Instance   8 | SCIP nb nodes      37  | SCIP time     3.78 \n",
      "             | GNN  nb nodes     391  | GNN  time     3.51 \n",
      "             | Gain          -956.76% | Gain          6.94%\n",
      "Instance   9 | SCIP nb nodes       1  | SCIP time     1.29 \n",
      "             | GNN  nb nodes       1  | GNN  time     1.13 \n",
      "             | Gain             0.00% | Gain         12.59%\n",
      "Instance  10 | SCIP nb nodes      59  | SCIP time     3.98 \n",
      "             | GNN  nb nodes     177  | GNN  time     2.75 \n",
      "             | Gain          -200.00% | Gain         30.96%\n",
      "Instance  11 | SCIP nb nodes       1  | SCIP time     1.50 \n",
      "             | GNN  nb nodes      15  | GNN  time     1.49 \n",
      "             | Gain         -1400.00% | Gain          0.51%\n",
      "Instance  12 | SCIP nb nodes       3  | SCIP time     2.17 \n",
      "             | GNN  nb nodes      25  | GNN  time     1.72 \n",
      "             | Gain          -733.33% | Gain         20.78%\n",
      "Instance  13 | SCIP nb nodes       3  | SCIP time     2.50 \n",
      "             | GNN  nb nodes      23  | GNN  time     1.81 \n",
      "             | Gain          -666.67% | Gain         27.66%\n",
      "Instance  14 | SCIP nb nodes     147  | SCIP time     5.41 \n",
      "             | GNN  nb nodes     349  | GNN  time     4.39 \n",
      "             | Gain          -137.41% | Gain         18.86%\n",
      "Instance  15 | SCIP nb nodes       1  | SCIP time     1.33 \n",
      "             | GNN  nb nodes       5  | GNN  time     1.28 \n",
      "             | Gain          -400.00% | Gain          3.52%\n",
      "Instance  16 | SCIP nb nodes     122  | SCIP time     4.74 \n",
      "             | GNN  nb nodes     223  | GNN  time     3.16 \n",
      "             | Gain           -82.79% | Gain         33.40%\n",
      "Instance  17 | SCIP nb nodes       1  | SCIP time     2.05 \n",
      "             | GNN  nb nodes      53  | GNN  time     1.96 \n",
      "             | Gain         -5200.00% | Gain          4.36%\n",
      "Instance  18 | SCIP nb nodes       1  | SCIP time     1.41 \n",
      "             | GNN  nb nodes       5  | GNN  time     1.24 \n",
      "             | Gain          -400.00% | Gain         12.35%\n",
      "Instance  19 | SCIP nb nodes      14  | SCIP time     2.88 \n",
      "             | GNN  nb nodes      93  | GNN  time     1.98 \n",
      "             | Gain          -564.29% | Gain         31.17%\n"
     ]
    }
   ],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=600, n_cols=1000, density=0.05)\n",
    "scip_parameters = {\n",
    "    \"separating/maxrounds\": 0,\n",
    "    \"presolving/maxrestarts\": 0,\n",
    "    \"limits/time\": 3600,\n",
    "}\n",
    "env = ecole.environment.Branching(\n",
    "    observation_function=ecole.observation.NodeBipartite(),\n",
    "    information_function={\n",
    "        \"nb_nodes\": ecole.reward.NNodes().cumsum(),\n",
    "        \"time\": ecole.reward.SolvingTime().cumsum(),\n",
    "    },\n",
    "    scip_params=scip_parameters,\n",
    ")\n",
    "default_env = ecole.environment.Configuring(\n",
    "    observation_function=None,\n",
    "    information_function={\n",
    "        \"nb_nodes\": ecole.reward.NNodes().cumsum(),\n",
    "        \"time\": ecole.reward.SolvingTime().cumsum(),\n",
    "    },\n",
    "    scip_params=scip_parameters,\n",
    ")\n",
    "\n",
    "for instance_count, instance in zip(range(20), instances):\n",
    "    # Run the GNN brancher\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (\n",
    "                torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE),\n",
    "                torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE),\n",
    "            )\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "    nb_nodes = info[\"nb_nodes\"]\n",
    "    time = info[\"time\"]\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance)\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "\n",
    "    print(\n",
    "        f\"Instance {instance_count: >3} | SCIP nb nodes    {int(default_info['nb_nodes']): >4d}  | SCIP time   {default_info['time']: >6.2f} \"\n",
    "    )\n",
    "    print(\n",
    "        f\"             | GNN  nb nodes    {int(nb_nodes): >4d}  | GNN  time   {time: >6.2f} \"\n",
    "    )\n",
    "    print(\n",
    "        f\"             | Gain         {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}% | Gain      {100*(1-time/default_info['time']): >8.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Gasse, M., Chtelat, D., Ferroni, N., Charlin, L. and Lodi, A. (2019). Exact combinatorial optimization with graph convolutional neural networks. In Advances in Neural Information Processing Systems (pp. 15580-15592)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
