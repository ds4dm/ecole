{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching with Imitation Learning and a GNN\n",
    "\n",
    "In this tutorial we will reproduce a simplified version of the paper of Gasse et al. (2019) on learning to branch with Ecole with `pytorch` and `pytorch geometric`. We collect strong branching examples on randomly generated maximum set covering instances, then train a graph neural network with bipartite state encodings to imitate the expert by classification. Finally, we will evaluate the quality of the policy.\n",
    "\n",
    "### Requirements\n",
    "This tutorial requires the following libraries. The version numbers used when writing this tutorial are given in parentheses.\n",
    "- `python` (3.8.2)\n",
    "- `numpy` (1.19.4)\n",
    "- `pytorch` (1.7.0)\n",
    "- `pytorch-geometric` (1.6.2)\n",
    "- `ecole` (0.4.2)\n",
    "\n",
    "The tutorial was designed with the provided version numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "Our first step will be to run explore-then-strong-branch on randomly generated maximum set covering instances, and save the branching decisions to build a dataset. We will also record the state of the branch-and-bound process as a bipartite graph, which is already implemented in Ecole with the same features as Gasse et al. (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ecole\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Ecole-provided set cover instance generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explore-then-strong-branch scheme described in the paper is not implemented by default in Ecole. In this scheme, to diversify the states in which we collect examples of strong branching behavior, we mostly follow a weak but cheap expert (pseudocost branching) and only occasionally call the strong expert (strong branching). This also ensures that samples are closer to being independent and identically distributed.\n",
    "\n",
    "This can be realized in Ecole by creating a custom observation function, which will randomly compute and return the pseudocost scores (cheap) or the strong branching scores (expensive). It also showcases extensibility in Ecole by showing how easily a custom observation function can be created and used, directly in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreThenStrongBranch:\n",
    "    \"\"\"\n",
    "    This custom observation function class will randomly return either strong branching scores (expensive expert) \n",
    "    or pseudocost scores (weak expert for exploration) when called at every node.\n",
    "    \"\"\"\n",
    "    def __init__(self, expert_probability):\n",
    "        self.expert_probability = expert_probability\n",
    "        self.pseudocosts_function = ecole.observation.Pseudocosts()\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "    \n",
    "    def before_reset(self, model):\n",
    "        \"\"\"\n",
    "        This function will be called at initialization of the environment (before dynamics are reset).\n",
    "        \"\"\"\n",
    "        self.pseudocosts_function.before_reset(model)\n",
    "        self.strong_branching_function.before_reset(model)\n",
    "    \n",
    "    def extract(self, model, done):\n",
    "        \"\"\"\n",
    "        Should we return strong branching or pseudocost scores at time node?\n",
    "        \"\"\"\n",
    "        probabilities = [1-self.expert_probability, self.expert_probability]\n",
    "        expert_chosen = bool(np.random.choice(np.arange(2), p=probabilities))\n",
    "        if expert_chosen:\n",
    "            return (self.strong_branching_function.extract(model, done), True)\n",
    "        else:\n",
    "            return (self.pseudocosts_function.extract(model, done), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the environment with the correct parameters (no restarts, 1h time limit, 5% expert sampling probability).\n",
    "\n",
    "Besides the (pseudocost or strong branching) scores, our environment will return the node bipartite graph representation of \n",
    "branch-and-bound states used in Gasse et al. (2019), using the `ecole.observation.NodeBipartite` observation function.\n",
    "On one side of that bipartite graph, nodes represent the variables of the problem, with a vector encoding features of \n",
    "that variable. On the other side of the bipartite graph, nodes represent the constraints of the problem, similarly with \n",
    "a vector encoding features of that constraint. An edge links a variable and a constraint node if the variable participates \n",
    "in that constraint, that is, its coefficient is nonzero in that constraint. The constraint coefficient is attached as an\n",
    "attribute of the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass custom SCIP parameters easily\n",
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "\n",
    "# Note how we can tuple observation functions to return complex state information\n",
    "env = ecole.environment.Branching(observation_function=(ExploreThenStrongBranch(expert_probability=0.05), \n",
    "                                                        ecole.observation.NodeBipartite()), \n",
    "                                  scip_params=scip_parameters)\n",
    "\n",
    "# This will seed the environment for reproducibility\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the instances, following the strong branching expert 5% of the time and saving its decision, until enough samples are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, 3 samples collected so far\n",
      "Episode 2, 16 samples collected so far\n",
      "Episode 3, 29 samples collected so far\n",
      "Episode 4, 35 samples collected so far\n",
      "Episode 5, 42 samples collected so far\n",
      "Episode 6, 46 samples collected so far\n",
      "Episode 7, 52 samples collected so far\n",
      "Episode 8, 63 samples collected so far\n",
      "Episode 9, 90 samples collected so far\n",
      "Episode 10, 93 samples collected so far\n",
      "Episode 11, 99 samples collected so far\n",
      "Episode 12, 115 samples collected so far\n",
      "Episode 13, 118 samples collected so far\n",
      "Episode 14, 119 samples collected so far\n",
      "Episode 15, 130 samples collected so far\n",
      "Episode 16, 130 samples collected so far\n",
      "Episode 17, 142 samples collected so far\n",
      "Episode 18, 146 samples collected so far\n",
      "Episode 19, 147 samples collected so far\n",
      "Episode 20, 151 samples collected so far\n",
      "Episode 21, 152 samples collected so far\n",
      "Episode 22, 159 samples collected so far\n",
      "Episode 23, 173 samples collected so far\n",
      "Episode 24, 179 samples collected so far\n",
      "Episode 25, 181 samples collected so far\n",
      "Episode 26, 216 samples collected so far\n",
      "Episode 27, 236 samples collected so far\n",
      "Episode 28, 539 samples collected so far\n",
      "Episode 29, 541 samples collected so far\n",
      "Episode 30, 555 samples collected so far\n",
      "Episode 31, 563 samples collected so far\n",
      "Episode 32, 582 samples collected so far\n",
      "Episode 33, 584 samples collected so far\n",
      "Episode 34, 584 samples collected so far\n",
      "Episode 35, 595 samples collected so far\n",
      "Episode 36, 652 samples collected so far\n",
      "Episode 37, 660 samples collected so far\n",
      "Episode 38, 664 samples collected so far\n",
      "Episode 39, 670 samples collected so far\n",
      "Episode 40, 716 samples collected so far\n",
      "Episode 41, 717 samples collected so far\n",
      "Episode 42, 720 samples collected so far\n",
      "Episode 43, 729 samples collected so far\n",
      "Episode 44, 729 samples collected so far\n",
      "Episode 45, 736 samples collected so far\n",
      "Episode 46, 736 samples collected so far\n",
      "Episode 47, 739 samples collected so far\n",
      "Episode 48, 743 samples collected so far\n",
      "Episode 49, 745 samples collected so far\n",
      "Episode 50, 769 samples collected so far\n",
      "Episode 51, 778 samples collected so far\n",
      "Episode 52, 784 samples collected so far\n",
      "Episode 53, 790 samples collected so far\n",
      "Episode 54, 791 samples collected so far\n",
      "Episode 55, 805 samples collected so far\n",
      "Episode 56, 808 samples collected so far\n",
      "Episode 57, 811 samples collected so far\n",
      "Episode 58, 839 samples collected so far\n",
      "Episode 59, 848 samples collected so far\n",
      "Episode 60, 848 samples collected so far\n",
      "Episode 61, 850 samples collected so far\n",
      "Episode 62, 877 samples collected so far\n",
      "Episode 63, 889 samples collected so far\n",
      "Episode 64, 892 samples collected so far\n",
      "Episode 65, 901 samples collected so far\n",
      "Episode 66, 908 samples collected so far\n",
      "Episode 67, 917 samples collected so far\n",
      "Episode 68, 921 samples collected so far\n",
      "Episode 69, 922 samples collected so far\n",
      "Episode 70, 975 samples collected so far\n",
      "Episode 71, 981 samples collected so far\n",
      "Episode 72, 981 samples collected so far\n",
      "Episode 73, 988 samples collected so far\n",
      "Episode 74, 991 samples collected so far\n",
      "Episode 75, 992 samples collected so far\n",
      "Episode 76, 1000 samples collected so far\n"
     ]
    }
   ],
   "source": [
    "episode_counter, sample_counter = 0, 0\n",
    "Path('samples/').mkdir(exist_ok=True)\n",
    "\n",
    "# We will solve problems (run episodes) until we have saved enough samples\n",
    "max_samples_reached = False\n",
    "while not max_samples_reached:\n",
    "    episode_counter += 1\n",
    "    \n",
    "    observation, action_set, _, done, _ = env.reset(next(instances))\n",
    "    while not done:\n",
    "        (scores, scores_are_expert), node_observation = observation\n",
    "        node_observation = (node_observation.row_features,\n",
    "                            (node_observation.edge_features.indices, \n",
    "                             node_observation.edge_features.values),\n",
    "                            node_observation.column_features)\n",
    "        \n",
    "        action = action_set[scores[action_set].argmax()]\n",
    "\n",
    "        # Only save samples if they are coming from the expert (strong branching)\n",
    "        if scores_are_expert and not max_samples_reached:\n",
    "            sample_counter += 1\n",
    "            data = [node_observation, action, action_set, scores]\n",
    "            filename = f'samples/sample_{sample_counter}.pkl'\n",
    "\n",
    "            with gzip.open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            # If we collected enough samples, we finish the current episode but stop saving samples\n",
    "            if sample_counter == MAX_SAMPLES:\n",
    "                max_samples_reached = True\n",
    "\n",
    "        observation, action_set, _, done, _ = env.step(action)\n",
    "    \n",
    "    print(f\"Episode {episode_counter}, {sample_counter} samples collected so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a GNN\n",
    "\n",
    "Our next step is to train a GNN classifier on these collected samples to predict similar choices to strong branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NB_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "EARLY_STOPPING = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define pytorch geometric data classes to handle the bipartite graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    \"\"\"\n",
    "    This class encode a node bipartite graph observation as returned by the `ecole.observation.NodeBipartite` \n",
    "    observation function in a format understood by the pytorch geometric data handlers.\n",
    "    \"\"\"\n",
    "    def __init__(self, constraint_features, edge_indices, edge_features, variable_features,\n",
    "                 candidates, candidate_choice, candidate_scores):\n",
    "        super().__init__()\n",
    "        self.constraint_features = constraint_features\n",
    "        self.edge_index = edge_indices\n",
    "        self.edge_attr = edge_features\n",
    "        self.variable_features = variable_features\n",
    "        self.candidates = candidates\n",
    "        self.nb_candidates = len(candidates)\n",
    "        self.candidate_choices = candidate_choice\n",
    "        self.candidate_scores = candidate_scores\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        \"\"\"\n",
    "        We overload the pytorch geometric method that tells how to increment indices when concatenating graphs \n",
    "        for those entries (edge index, candidates) for which this is not obvious.\n",
    "        \"\"\"\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.constraint_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n",
    "\n",
    "\n",
    "class GraphDataset(torch_geometric.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class encodes a collection of graphs, as well as a method to load such graphs from the disk.\n",
    "    It can be used in turn by the data loaders provided by pytorch geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_files):\n",
    "        super().__init__(root=None, transform=None, pre_transform=None)\n",
    "        self.sample_files = sample_files\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sample_files)\n",
    "\n",
    "    def get(self, index):\n",
    "        \"\"\"\n",
    "        This method loads a node bipartite graph observation as saved on the disk during data collection.\n",
    "        \"\"\"\n",
    "        with gzip.open(self.sample_files[index], 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action, sample_action_set, sample_scores = sample\n",
    "\n",
    "        constraint_features, (edge_indices, edge_features), variable_features = sample_observation\n",
    "        constraint_features = torch.from_numpy(constraint_features.astype(np.float32))\n",
    "        edge_indices = torch.from_numpy(edge_indices.astype(np.int64))\n",
    "        edge_features = torch.from_numpy(edge_features.astype(np.float32)).view(-1, 1)\n",
    "        variable_features = torch.from_numpy(variable_features.astype(np.float32))\n",
    "        \n",
    "        # We note on which variables we were allowed to branch, the scores as well as the choice \n",
    "        # taken by strong branching (relative to the candidates)\n",
    "        candidates = torch.LongTensor(np.array(sample_action_set, dtype=np.int32))\n",
    "        candidate_scores = torch.FloatTensor([sample_scores[j] for j in candidates])\n",
    "        candidate_choice = torch.where(candidates == sample_action)[0][0]\n",
    "\n",
    "        graph = BipartiteNodeData(constraint_features, edge_indices, edge_features, variable_features,\n",
    "                                  candidates, candidate_choice, candidate_scores)\n",
    "        \n",
    "        # We must tell pytorch geometric how many nodes there are, for indexing purposes\n",
    "        graph.num_nodes = constraint_features.shape[0]+variable_features.shape[0]\n",
    "        \n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then prepare the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = [str(path) for path in Path('samples/').glob('sample_*.pkl')]\n",
    "train_files = sample_files[:int(0.8*len(sample_files))]\n",
    "valid_files = sample_files[int(0.8*len(sample_files)):]\n",
    "\n",
    "train_data = GraphDataset(train_files)\n",
    "train_loader = torch_geometric.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_data = GraphDataset(valid_files)\n",
    "valid_loader = torch_geometric.data.DataLoader(valid_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our graph neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNPolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        emb_size = 64\n",
    "        cons_nfeats = 5\n",
    "        edge_nfeats = 1\n",
    "        var_nfeats = 19\n",
    "\n",
    "        # CONSTRAINT EMBEDDING\n",
    "        self.cons_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(cons_nfeats),\n",
    "            torch.nn.Linear(cons_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # EDGE EMBEDDING\n",
    "        self.edge_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(edge_nfeats),\n",
    "        )\n",
    "\n",
    "        # VARIABLE EMBEDDING\n",
    "        self.var_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(var_nfeats),\n",
    "            torch.nn.Linear(var_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_v_to_c = BipartiteGraphConvolution()\n",
    "        self.conv_c_to_v = BipartiteGraphConvolution()\n",
    "\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, constraint_features, edge_indices, edge_features, variable_features):\n",
    "        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n",
    "        \n",
    "        # First step: linear embedding layers to a common dimension (64)\n",
    "        constraint_features = self.cons_embedding(constraint_features)\n",
    "        edge_features = self.edge_embedding(edge_features)\n",
    "        variable_features = self.var_embedding(variable_features)\n",
    "\n",
    "        # Two half convolutions\n",
    "        constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, edge_features, constraint_features)\n",
    "        variable_features = self.conv_c_to_v(constraint_features, edge_indices, edge_features, variable_features)\n",
    "\n",
    "        # A final MLP on the variable features\n",
    "        output = self.output_module(variable_features).squeeze(-1)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n",
    "    \"\"\"\n",
    "    The bipartite graph convolution is already provided by pytorch geometric and we merely need \n",
    "    to provide the exact form of the messages being passed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__('add')\n",
    "        emb_size = 64\n",
    "        \n",
    "        self.feature_module_left = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        self.feature_module_edge = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_right = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_final = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        \n",
    "        self.post_conv_module = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size)\n",
    "        )\n",
    "\n",
    "        # output_layers\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, left_features, edge_indices, edge_features, right_features):\n",
    "        \"\"\"\n",
    "        This method sends the messages, computed in the message method.\n",
    "        \"\"\"\n",
    "        output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n",
    "                                node_features=(left_features, right_features), edge_features=edge_features)\n",
    "        return self.output_module(torch.cat([self.post_conv_module(output), right_features], dim=-1))\n",
    "\n",
    "    def message(self, node_features_i, node_features_j, edge_features):\n",
    "        output = self.feature_module_final(self.feature_module_left(node_features_i) \n",
    "                                           + self.feature_module_edge(edge_features) \n",
    "                                           + self.feature_module_right(node_features_j))\n",
    "        return output\n",
    "    \n",
    "\n",
    "policy = GNNPolicy().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we can predict a probability distribution over actions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0127, 0.0130, 0.0126, 0.0126,\n",
      "        0.0127, 0.0129, 0.0126, 0.0127, 0.0128, 0.0127, 0.0127, 0.0128, 0.0129,\n",
      "        0.0126, 0.0131, 0.0126, 0.0127, 0.0128, 0.0128, 0.0129, 0.0131, 0.0127,\n",
      "        0.0128, 0.0127, 0.0126, 0.0128, 0.0128, 0.0127, 0.0129, 0.0129, 0.0126,\n",
      "        0.0127, 0.0126, 0.0128, 0.0128, 0.0130, 0.0130, 0.0129, 0.0128, 0.0127,\n",
      "        0.0129, 0.0129, 0.0130, 0.0127, 0.0126, 0.0130, 0.0127, 0.0129, 0.0130,\n",
      "        0.0129, 0.0126, 0.0130, 0.0128, 0.0128, 0.0130, 0.0130, 0.0131, 0.0126,\n",
      "        0.0130, 0.0130, 0.0130, 0.0126, 0.0130, 0.0129, 0.0130, 0.0131, 0.0130,\n",
      "        0.0130, 0.0129, 0.0126, 0.0130, 0.0128, 0.0129], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "observation = train_data[0].to(DEVICE)\n",
    "\n",
    "logits = policy(observation.constraint_features, observation.edge_index, observation.edge_attr, observation.variable_features)\n",
    "action_distribution = F.softmax(logits[observation.candidates], dim=-1)\n",
    "\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, with randomly initialized weights, the initial distributions tend to be close to uniform.\n",
    "Next, we will define two helper functions: one to train or evaluate the model on a whole epoch and compute metrics for monitoring, and one for padding tensors when doing predictions on a batch of graphs of potentially different number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(policy, data_loader, optimizer=None):\n",
    "    \"\"\"\n",
    "    This function will process a whole epoch of training or validation, depending on whether an optimizer is provided.\n",
    "    \"\"\"\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            # Compute the logits (i.e. pre-softmax activations) according to the policy on the concatenated graphs\n",
    "            logits = policy(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n",
    "            # Index the results by the candidates, and split and pad them\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "            # Compute the usual cross-entropy classification loss\n",
    "            loss = F.cross_entropy(logits, batch.candidate_choices)\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            true_scores = pad_tensor(batch.candidate_scores, batch.nb_candidates)\n",
    "            true_bestscore = true_scores.max(dim=-1, keepdims=True).values\n",
    "            \n",
    "            predicted_bestindex = logits.max(dim=-1, keepdims=True).indices\n",
    "            accuracy = (true_scores.gather(-1, predicted_bestindex) == true_bestscore).float().mean().item()\n",
    "\n",
    "            mean_loss += loss.item() * batch.num_graphs\n",
    "            mean_acc += accuracy * batch.num_graphs\n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_acc /= n_samples_processed\n",
    "    return mean_loss, mean_acc\n",
    "\n",
    "\n",
    "def pad_tensor(input_, pad_sizes, pad_value=-1e8):\n",
    "    \"\"\"\n",
    "    This utility function splits a tensor and pads each split to make them all the same size, then stacks them.\n",
    "    \"\"\"\n",
    "    max_pad_size = pad_sizes.max()\n",
    "    output = input_.split(pad_sizes.cpu().numpy().tolist())\n",
    "    output = torch.stack([F.pad(slice_, (0, max_pad_size-slice_.size(0)), 'constant', pad_value)\n",
    "                          for slice_ in output], dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can actually create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 4.201, accuracy 0.236\n",
      "Valid loss: 3.869, accuracy 0.365\n",
      "Epoch 2\n",
      "Train loss: 3.612, accuracy 0.399\n",
      "Valid loss: 3.532, accuracy 0.430\n",
      "Epoch 3\n",
      "Train loss: 3.448, accuracy 0.445\n",
      "Valid loss: 3.476, accuracy 0.455\n",
      "Epoch 4\n",
      "Train loss: 3.417, accuracy 0.453\n",
      "Valid loss: 3.415, accuracy 0.455\n",
      "Epoch 5\n",
      "Train loss: 3.402, accuracy 0.456\n",
      "Valid loss: 3.390, accuracy 0.445\n",
      "Epoch 6\n",
      "Train loss: 3.400, accuracy 0.458\n",
      "Valid loss: 3.380, accuracy 0.450\n",
      "Epoch 7\n",
      "Train loss: 3.383, accuracy 0.458\n",
      "Valid loss: 3.394, accuracy 0.465\n",
      "Epoch 8\n",
      "Train loss: 3.373, accuracy 0.466\n",
      "Valid loss: 3.458, accuracy 0.440\n",
      "Epoch 9\n",
      "Train loss: 3.372, accuracy 0.459\n",
      "Valid loss: 3.471, accuracy 0.435\n",
      "Epoch 10\n",
      "Train loss: 3.370, accuracy 0.463\n",
      "Valid loss: 3.397, accuracy 0.460\n",
      "Epoch 11\n",
      "Train loss: 3.351, accuracy 0.466\n",
      "Valid loss: 3.411, accuracy 0.455\n",
      "Epoch 12\n",
      "Train loss: 3.350, accuracy 0.468\n",
      "Valid loss: 3.396, accuracy 0.445\n",
      "Epoch 13\n",
      "Train loss: 3.361, accuracy 0.448\n",
      "Valid loss: 3.360, accuracy 0.455\n",
      "Epoch 14\n",
      "Train loss: 3.331, accuracy 0.464\n",
      "Valid loss: 3.370, accuracy 0.460\n",
      "Epoch 15\n",
      "Train loss: 3.325, accuracy 0.477\n",
      "Valid loss: 3.357, accuracy 0.470\n",
      "Epoch 16\n",
      "Train loss: 3.283, accuracy 0.461\n",
      "Valid loss: 3.326, accuracy 0.440\n",
      "Epoch 17\n",
      "Train loss: 3.248, accuracy 0.456\n",
      "Valid loss: 3.323, accuracy 0.435\n",
      "Epoch 18\n",
      "Train loss: 3.247, accuracy 0.468\n",
      "Valid loss: 3.328, accuracy 0.455\n",
      "Epoch 19\n",
      "Train loss: 3.263, accuracy 0.465\n",
      "Valid loss: 3.278, accuracy 0.435\n",
      "Epoch 20\n",
      "Train loss: 3.194, accuracy 0.470\n",
      "Valid loss: 3.300, accuracy 0.430\n",
      "Epoch 21\n",
      "Train loss: 3.187, accuracy 0.470\n",
      "Valid loss: 3.326, accuracy 0.390\n",
      "Epoch 22\n",
      "Train loss: 3.210, accuracy 0.463\n",
      "Valid loss: 3.330, accuracy 0.405\n",
      "Epoch 23\n",
      "Train loss: 3.201, accuracy 0.463\n",
      "Valid loss: 3.225, accuracy 0.450\n",
      "Epoch 24\n",
      "Train loss: 3.166, accuracy 0.465\n",
      "Valid loss: 3.311, accuracy 0.420\n",
      "Epoch 25\n",
      "Train loss: 3.173, accuracy 0.456\n",
      "Valid loss: 3.290, accuracy 0.415\n",
      "Epoch 26\n",
      "Train loss: 3.187, accuracy 0.472\n",
      "Valid loss: 3.229, accuracy 0.435\n",
      "Epoch 27\n",
      "Train loss: 3.124, accuracy 0.460\n",
      "Valid loss: 3.216, accuracy 0.440\n",
      "Epoch 28\n",
      "Train loss: 3.137, accuracy 0.454\n",
      "Valid loss: 3.302, accuracy 0.445\n",
      "Epoch 29\n",
      "Train loss: 3.115, accuracy 0.476\n",
      "Valid loss: 3.251, accuracy 0.415\n",
      "Epoch 30\n",
      "Train loss: 3.115, accuracy 0.471\n",
      "Valid loss: 3.310, accuracy 0.405\n",
      "Epoch 31\n",
      "Train loss: 3.119, accuracy 0.474\n",
      "Valid loss: 3.220, accuracy 0.430\n",
      "Epoch 32\n",
      "Train loss: 3.047, accuracy 0.472\n",
      "Valid loss: 3.323, accuracy 0.480\n",
      "Epoch 33\n",
      "Train loss: 3.123, accuracy 0.464\n",
      "Valid loss: 3.263, accuracy 0.440\n",
      "Epoch 34\n",
      "Train loss: 3.069, accuracy 0.472\n",
      "Valid loss: 3.345, accuracy 0.420\n",
      "Epoch 35\n",
      "Train loss: 3.083, accuracy 0.491\n",
      "Valid loss: 3.196, accuracy 0.435\n",
      "Epoch 36\n",
      "Train loss: 3.055, accuracy 0.474\n",
      "Valid loss: 3.376, accuracy 0.460\n",
      "Epoch 37\n",
      "Train loss: 3.078, accuracy 0.487\n",
      "Valid loss: 3.299, accuracy 0.410\n",
      "Epoch 38\n",
      "Train loss: 3.028, accuracy 0.477\n",
      "Valid loss: 3.231, accuracy 0.425\n",
      "Epoch 39\n",
      "Train loss: 3.021, accuracy 0.484\n",
      "Valid loss: 3.328, accuracy 0.415\n",
      "Epoch 40\n",
      "Train loss: 3.033, accuracy 0.482\n",
      "Valid loss: 3.221, accuracy 0.425\n",
      "Epoch 41\n",
      "Train loss: 2.991, accuracy 0.484\n",
      "Valid loss: 3.275, accuracy 0.430\n",
      "Epoch 42\n",
      "Train loss: 3.020, accuracy 0.472\n",
      "Valid loss: 3.345, accuracy 0.440\n",
      "Epoch 43\n",
      "Train loss: 2.991, accuracy 0.485\n",
      "Valid loss: 3.264, accuracy 0.420\n",
      "Epoch 44\n",
      "Train loss: 2.980, accuracy 0.471\n",
      "Valid loss: 3.296, accuracy 0.430\n",
      "Epoch 45\n",
      "Train loss: 2.981, accuracy 0.476\n",
      "Valid loss: 3.269, accuracy 0.460\n",
      "Epoch 46\n",
      "Train loss: 3.032, accuracy 0.487\n",
      "Valid loss: 3.230, accuracy 0.430\n",
      "Epoch 47\n",
      "Train loss: 2.999, accuracy 0.487\n",
      "Valid loss: 3.329, accuracy 0.430\n",
      "Epoch 48\n",
      "Train loss: 2.946, accuracy 0.491\n",
      "Valid loss: 3.271, accuracy 0.420\n",
      "Epoch 49\n",
      "Train loss: 2.920, accuracy 0.481\n",
      "Valid loss: 3.242, accuracy 0.435\n",
      "Epoch 50\n",
      "Train loss: 2.957, accuracy 0.470\n",
      "Valid loss: 3.326, accuracy 0.455\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    train_loss, train_acc = process(policy, train_loader, optimizer)\n",
    "    print(f\"Train loss: {train_loss:0.3f}, accuracy {train_acc:0.3f}\" )\n",
    "\n",
    "    valid_loss, valid_acc = process(policy, valid_loader, None)\n",
    "    print(f\"Valid loss: {valid_loss:0.3f}, accuracy {valid_acc:0.3f}\" )\n",
    "\n",
    "torch.save(policy.state_dict(), 'trained_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluation\n",
    "\n",
    "Finally, we can evaluate the performance of the model. We first define appropriate environments. For benchmarking purposes, we include a trivial environment that merely runs SCIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "env = ecole.environment.Branching(observation_function=ecole.observation.NodeBipartite(), \n",
    "                                  information_function={\"nb_nodes\": ecole.reward.NNodes(), \n",
    "                                                        \"time\": ecole.reward.SolvingTime()}, \n",
    "                                  scip_params=scip_parameters)\n",
    "default_env = ecole.environment.Configuring(observation_function=None,\n",
    "                                            information_function={\"nb_nodes\": ecole.reward.NNodes(), \n",
    "                                                                  \"time\": ecole.reward.SolvingTime()}, \n",
    "                                            scip_params=scip_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply follow the environments, taking steps appropriately according to the GNN policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes      25  | SCIP time     4.51 \n",
      "             | GNN  nb nodes     227  | GNN  time     3.27 \n",
      "             | Gain          -808.00% | Gain         27.49%\n",
      "Instance   1 | SCIP nb nodes       3  | SCIP time     2.04 \n",
      "             | GNN  nb nodes      43  | GNN  time     1.86 \n",
      "             | Gain         -1333.33% | Gain          8.82%\n",
      "Instance   2 | SCIP nb nodes      27  | SCIP time     5.65 \n",
      "             | GNN  nb nodes     179  | GNN  time     3.93 \n",
      "             | Gain          -562.96% | Gain         30.44%\n",
      "Instance   3 | SCIP nb nodes      59  | SCIP time     6.40 \n",
      "             | GNN  nb nodes     257  | GNN  time     4.50 \n",
      "             | Gain          -335.59% | Gain         29.69%\n",
      "Instance   4 | SCIP nb nodes      21  | SCIP time     3.09 \n",
      "             | GNN  nb nodes      57  | GNN  time     2.26 \n",
      "             | Gain          -171.43% | Gain         26.86%\n",
      "Instance   5 | SCIP nb nodes      37  | SCIP time     5.22 \n",
      "             | GNN  nb nodes     163  | GNN  time     3.70 \n",
      "             | Gain          -340.54% | Gain         29.12%\n",
      "Instance   6 | SCIP nb nodes     806  | SCIP time    10.83 \n",
      "             | GNN  nb nodes    1821  | GNN  time    12.17 \n",
      "             | Gain          -125.93% | Gain        -12.37%\n",
      "Instance   7 | SCIP nb nodes      29  | SCIP time     4.47 \n",
      "             | GNN  nb nodes     168  | GNN  time     3.34 \n",
      "             | Gain          -479.31% | Gain         25.28%\n",
      "Instance   8 | SCIP nb nodes     661  | SCIP time     7.10 \n",
      "             | GNN  nb nodes    1049  | GNN  time     7.70 \n",
      "             | Gain           -58.70% | Gain         -8.45%\n",
      "Instance   9 | SCIP nb nodes     301  | SCIP time     8.48 \n",
      "             | GNN  nb nodes     627  | GNN  time     6.44 \n",
      "             | Gain          -108.31% | Gain         24.06%\n",
      "Instance  10 | SCIP nb nodes       1  | SCIP time     2.70 \n",
      "             | GNN  nb nodes      21  | GNN  time     1.95 \n",
      "             | Gain         -2000.00% | Gain         27.78%\n",
      "Instance  11 | SCIP nb nodes     207  | SCIP time     6.99 \n",
      "             | GNN  nb nodes     561  | GNN  time     5.78 \n",
      "             | Gain          -171.01% | Gain         17.31%\n",
      "Instance  12 | SCIP nb nodes     457  | SCIP time     8.04 \n",
      "             | GNN  nb nodes     910  | GNN  time     7.18 \n",
      "             | Gain           -99.12% | Gain         10.70%\n",
      "Instance  13 | SCIP nb nodes     575  | SCIP time     7.79 \n",
      "             | GNN  nb nodes    1247  | GNN  time     7.83 \n",
      "             | Gain          -116.87% | Gain         -0.51%\n",
      "Instance  14 | SCIP nb nodes      27  | SCIP time     5.00 \n",
      "             | GNN  nb nodes     141  | GNN  time     3.56 \n",
      "             | Gain          -422.22% | Gain         28.80%\n",
      "Instance  15 | SCIP nb nodes    1631  | SCIP time    13.93 \n",
      "             | GNN  nb nodes    7473  | GNN  time    37.99 \n",
      "             | Gain          -358.19% | Gain       -172.72%\n",
      "Instance  16 | SCIP nb nodes      13  | SCIP time     3.64 \n",
      "             | GNN  nb nodes      81  | GNN  time     2.61 \n",
      "             | Gain          -523.08% | Gain         28.30%\n",
      "Instance  17 | SCIP nb nodes      75  | SCIP time     5.60 \n",
      "             | GNN  nb nodes     240  | GNN  time     4.01 \n",
      "             | Gain          -220.00% | Gain         28.39%\n",
      "Instance  18 | SCIP nb nodes      13  | SCIP time     3.84 \n",
      "             | GNN  nb nodes     133  | GNN  time     2.94 \n",
      "             | Gain          -923.08% | Gain         23.44%\n",
      "Instance  19 | SCIP nb nodes     181  | SCIP time     6.17 \n",
      "             | GNN  nb nodes     423  | GNN  time     4.62 \n",
      "             | Gain          -133.70% | Gain         25.12%\n"
     ]
    }
   ],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "for instance_count, instance in zip(range(20), instances):\n",
    "    # Run the GNN brancher\n",
    "    nb_nodes, time = 0, 0\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    nb_nodes += info['nb_nodes']\n",
    "    time += info['time']\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                           torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE), \n",
    "                           torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                           torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE))\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "        nb_nodes += info['nb_nodes']\n",
    "        time += info['time']\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance)\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "    \n",
    "    print(f\"Instance {instance_count: >3} | SCIP nb nodes    {int(default_info['nb_nodes']): >4d}  | SCIP time   {default_info['time']: >6.2f} \")\n",
    "    print(f\"             | GNN  nb nodes    {int(nb_nodes): >4d}  | GNN  time   {time: >6.2f} \")\n",
    "    print(f\"             | Gain         {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}% | Gain      {100*(1-time/default_info['time']): >8.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate on instances larger and harder than those trained on, say with 600 rather than 500 constraints.\n",
    "In addition, we showcase that the cumulative number of nodes and time required to solve an instance can also be computed directly using the `.cumsum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes     143  | SCIP time     7.08 \n",
      "             | GNN  nb nodes     505  | GNN  time     5.86 \n",
      "             | Gain          -253.15% | Gain         17.23%\n",
      "Instance   1 | SCIP nb nodes      81  | SCIP time     8.72 \n",
      "             | GNN  nb nodes     407  | GNN  time     6.75 \n",
      "             | Gain          -402.47% | Gain         22.59%\n",
      "Instance   2 | SCIP nb nodes     320  | SCIP time     8.05 \n",
      "             | GNN  nb nodes    1585  | GNN  time    11.00 \n",
      "             | Gain          -395.31% | Gain        -36.65%\n",
      "Instance   3 | SCIP nb nodes       1  | SCIP time     2.13 \n",
      "             | GNN  nb nodes       7  | GNN  time     2.32 \n",
      "             | Gain          -600.00% | Gain         -8.92%\n",
      "Instance   4 | SCIP nb nodes     163  | SCIP time     8.45 \n",
      "             | GNN  nb nodes    1503  | GNN  time    10.84 \n",
      "             | Gain          -822.09% | Gain        -28.28%\n",
      "Instance   5 | SCIP nb nodes     409  | SCIP time    10.38 \n",
      "             | GNN  nb nodes     873  | GNN  time     9.52 \n",
      "             | Gain          -113.45% | Gain          8.29%\n",
      "Instance   6 | SCIP nb nodes       5  | SCIP time     3.46 \n",
      "             | GNN  nb nodes      77  | GNN  time     3.10 \n",
      "             | Gain         -1440.00% | Gain         10.40%\n",
      "Instance   7 | SCIP nb nodes     229  | SCIP time     9.70 \n",
      "             | GNN  nb nodes     529  | GNN  time     7.70 \n",
      "             | Gain          -131.00% | Gain         20.62%\n",
      "Instance   8 | SCIP nb nodes      15  | SCIP time     4.47 \n",
      "             | GNN  nb nodes     132  | GNN  time     3.72 \n",
      "             | Gain          -780.00% | Gain         16.78%\n",
      "Instance   9 | SCIP nb nodes      19  | SCIP time     4.92 \n",
      "             | GNN  nb nodes     213  | GNN  time     4.02 \n",
      "             | Gain         -1021.05% | Gain         18.29%\n",
      "Instance  10 | SCIP nb nodes     183  | SCIP time     7.27 \n",
      "             | GNN  nb nodes     292  | GNN  time     5.07 \n",
      "             | Gain           -59.56% | Gain         30.26%\n",
      "Instance  11 | SCIP nb nodes    3768  | SCIP time    25.76 \n",
      "             | GNN  nb nodes    16672  | GNN  time    95.62 \n",
      "             | Gain          -342.46% | Gain       -271.20%\n",
      "Instance  12 | SCIP nb nodes     225  | SCIP time     7.25 \n",
      "             | GNN  nb nodes     439  | GNN  time     5.61 \n",
      "             | Gain           -95.11% | Gain         22.62%\n",
      "Instance  13 | SCIP nb nodes     335  | SCIP time     9.21 \n",
      "             | GNN  nb nodes    1007  | GNN  time     9.27 \n",
      "             | Gain          -200.60% | Gain         -0.65%\n",
      "Instance  14 | SCIP nb nodes     147  | SCIP time     8.58 \n",
      "             | GNN  nb nodes     891  | GNN  time     8.69 \n",
      "             | Gain          -506.12% | Gain         -1.28%\n",
      "Instance  15 | SCIP nb nodes      81  | SCIP time     7.48 \n",
      "             | GNN  nb nodes     275  | GNN  time     4.87 \n",
      "             | Gain          -239.51% | Gain         34.89%\n",
      "Instance  16 | SCIP nb nodes     251  | SCIP time    10.51 \n",
      "             | GNN  nb nodes     931  | GNN  time    10.53 \n",
      "             | Gain          -270.92% | Gain         -0.19%\n",
      "Instance  17 | SCIP nb nodes     942  | SCIP time    12.32 \n",
      "             | GNN  nb nodes     890  | GNN  time     9.78 \n",
      "             | Gain             5.52% | Gain         20.62%\n",
      "Instance  18 | SCIP nb nodes       5  | SCIP time     3.92 \n",
      "             | GNN  nb nodes      77  | GNN  time     3.00 \n",
      "             | Gain         -1440.00% | Gain         23.47%\n",
      "Instance  19 | SCIP nb nodes      13  | SCIP time     5.22 \n",
      "             | GNN  nb nodes     195  | GNN  time     4.26 \n",
      "             | Gain         -1400.00% | Gain         18.39%\n"
     ]
    }
   ],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=600, n_cols=1000, density=0.05)\n",
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "env = ecole.environment.Branching(observation_function=ecole.observation.NodeBipartite(), \n",
    "                                  information_function={\"nb_nodes\": ecole.reward.NNodes().cumsum(), \n",
    "                                                        \"time\": ecole.reward.SolvingTime().cumsum()}, \n",
    "                                  scip_params=scip_parameters)\n",
    "default_env = ecole.environment.Configuring(observation_function=None,\n",
    "                                            information_function={\"nb_nodes\": ecole.reward.NNodes().cumsum(), \n",
    "                                                                  \"time\": ecole.reward.SolvingTime().cumsum()}, \n",
    "                                            scip_params=scip_parameters)\n",
    "\n",
    "for instance_count, instance in zip(range(20), instances):\n",
    "    # Run the GNN brancher\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                           torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE), \n",
    "                           torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                           torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE))\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "    nb_nodes = info['nb_nodes']\n",
    "    time = info['time']\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance)\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "\n",
    "    print(f\"Instance {instance_count: >3} | SCIP nb nodes    {int(default_info['nb_nodes']): >4d}  | SCIP time   {default_info['time']: >6.2f} \")\n",
    "    print(f\"             | GNN  nb nodes    {int(nb_nodes): >4d}  | GNN  time   {time: >6.2f} \")\n",
    "    print(f\"             | Gain         {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}% | Gain      {100*(1-time/default_info['time']): >8.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Gasse, M., Ch√©telat, D., Ferroni, N., Charlin, L. and Lodi, A. (2019). Exact combinatorial optimization with graph convolutional neural networks. In Advances in Neural Information Processing Systems (pp. 15580-15592)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
