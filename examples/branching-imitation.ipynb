{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching with Imitation Learning and a GNN\n",
    "\n",
    "In this tutorial we will reproduce a simplified version of the paper of Gasse et al. (2019) on learning to branch with Ecole with `pytorch` and `pytorch geometric`. We collect strong branching examples on randomly generated maximum set covering instances, then train a graph neural network with bipartite state encodings to imitate the expert by classification. Finally, we will evaluate the quality of the policy.\n",
    "\n",
    "This tutorial requires the following libraries. The version numbers used when writing this tutorial are given in parentheses.\n",
    "- `python` (3.8.2)\n",
    "- `numpy` (1.19.4)\n",
    "- `pytorch` (1.7.0)\n",
    "- `pytorch-geometric` (1.6.2)\n",
    "- `ecole` (0.4.2)\n",
    "\n",
    "The tutorial was designed with the provided version numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "Our first step will be to run explore-then-strong-branch on randomly generated maximum set covering instances, and save the branching decisions to build a dataset. We will also record the state of the branch-and-bound process as a bipartite graph, which is already implemented in Ecole with the same features as Gasse et al. (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ecole\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Ecole-provided set cover instance generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explore-then-strong-branch scheme described in the paper is not implemented by default in Ecole, but we can easily write this branching rule in python, which showcasees the flexibility of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreThenStrongBranch:\n",
    "    def __init__(self, expert_probability):\n",
    "        self.expert_probability = expert_probability\n",
    "        self.pseudocosts_function = ecole.observation.Pseudocosts()\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "    \n",
    "    def before_reset(self, model):\n",
    "        self.pseudocosts_function.before_reset(model)\n",
    "        self.strong_branching_function.before_reset(model)\n",
    "    \n",
    "    def extract(self, model, done):\n",
    "        probabilities = [1-self.expert_probability, self.expert_probability]\n",
    "        expert_chosen = bool(np.random.choice(np.arange(2), p=probabilities))\n",
    "        if expert_chosen:\n",
    "            return (self.strong_branching_function.extract(model, done), True)\n",
    "        else:\n",
    "            return (self.pseudocosts_function.extract(model, done), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the environment with the correct parameters (no restarts, 1h time limit, 5% expert sampling probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "env = ecole.environment.Branching(observation_function=(ExploreThenStrongBranch(expert_probability=0.05), \n",
    "                                                        ecole.observation.NodeBipartite()), \n",
    "                                  scip_params=scip_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the instances, following the strong branching expert 5% of the time and saving its decision, until enough samples are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, 2 samples collected so far\n",
      "Episode 2, 12 samples collected so far\n",
      "Episode 3, 18 samples collected so far\n",
      "Episode 4, 25 samples collected so far\n",
      "Episode 5, 40 samples collected so far\n",
      "Episode 6, 67 samples collected so far\n",
      "Episode 7, 72 samples collected so far\n",
      "Episode 8, 75 samples collected so far\n",
      "Episode 9, 77 samples collected so far\n",
      "Episode 10, 79 samples collected so far\n",
      "Episode 11, 111 samples collected so far\n",
      "Episode 12, 136 samples collected so far\n",
      "Episode 13, 153 samples collected so far\n",
      "Episode 14, 156 samples collected so far\n",
      "Episode 15, 157 samples collected so far\n",
      "Episode 16, 158 samples collected so far\n",
      "Episode 17, 165 samples collected so far\n",
      "Episode 18, 182 samples collected so far\n",
      "Episode 19, 193 samples collected so far\n",
      "Episode 20, 221 samples collected so far\n",
      "Episode 21, 235 samples collected so far\n",
      "Episode 22, 252 samples collected so far\n",
      "Episode 23, 258 samples collected so far\n",
      "Episode 24, 262 samples collected so far\n",
      "Episode 25, 263 samples collected so far\n",
      "Episode 26, 269 samples collected so far\n",
      "Episode 27, 270 samples collected so far\n",
      "Episode 28, 275 samples collected so far\n",
      "Episode 29, 283 samples collected so far\n",
      "Episode 30, 283 samples collected so far\n",
      "Episode 31, 283 samples collected so far\n",
      "Episode 32, 287 samples collected so far\n",
      "Episode 33, 289 samples collected so far\n",
      "Episode 34, 289 samples collected so far\n",
      "Episode 35, 299 samples collected so far\n",
      "Episode 36, 305 samples collected so far\n",
      "Episode 37, 325 samples collected so far\n",
      "Episode 38, 325 samples collected so far\n",
      "Episode 39, 325 samples collected so far\n",
      "Episode 40, 329 samples collected so far\n",
      "Episode 41, 337 samples collected so far\n",
      "Episode 42, 793 samples collected so far\n",
      "Episode 43, 812 samples collected so far\n",
      "Episode 44, 818 samples collected so far\n",
      "Episode 45, 819 samples collected so far\n",
      "Episode 46, 821 samples collected so far\n",
      "Episode 47, 829 samples collected so far\n",
      "Episode 48, 835 samples collected so far\n",
      "Episode 49, 836 samples collected so far\n",
      "Episode 50, 837 samples collected so far\n",
      "Episode 51, 843 samples collected so far\n",
      "Episode 52, 844 samples collected so far\n",
      "Episode 53, 872 samples collected so far\n",
      "Episode 54, 876 samples collected so far\n",
      "Episode 55, 890 samples collected so far\n",
      "Episode 56, 1000 samples collected so far\n"
     ]
    }
   ],
   "source": [
    "episode_counter = 0\n",
    "sample_counter = 0\n",
    "max_samples_reached = False\n",
    "Path('samples/').mkdir(exist_ok=True)\n",
    "env.seed(0)\n",
    "\n",
    "while not max_samples_reached:\n",
    "    episode_counter += 1\n",
    "    \n",
    "    observation, action_set, _, done, _ = env.reset(next(instances))\n",
    "    while not done:\n",
    "        scores, node_observation = observation\n",
    "        scores, scores_are_expert = scores\n",
    "        node_observation = (node_observation.row_features,\n",
    "                            (node_observation.edge_features.indices, \n",
    "                             node_observation.edge_features.values),\n",
    "                            node_observation.column_features)\n",
    "\n",
    "        action = action_set[scores[action_set].argmax()]\n",
    "\n",
    "        if scores_are_expert and not max_samples_reached:\n",
    "            sample_counter += 1\n",
    "            data = [node_observation, action, action_set, scores]\n",
    "            filename = f'samples/sample_{sample_counter}.pkl'\n",
    "\n",
    "            with gzip.open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            if sample_counter == MAX_SAMPLES:\n",
    "                max_samples_reached = True\n",
    "\n",
    "        observation, action_set, _, done, _ = env.step(action)\n",
    "    \n",
    "    print(f\"Episode {episode_counter}, {sample_counter} samples collected so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a GNN\n",
    "\n",
    "Our next step is to train a GNN classifier on these collected samples to predict similar choices to strong branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "EARLY_STOPPING = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define pytorch geometric data classes to handle the bipartite graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    def __init__(self, constraint_features, edge_indices, edge_features, variable_features,\n",
    "                 candidates, candidate_choice, candidate_scores):\n",
    "        super().__init__()\n",
    "        self.constraint_features = constraint_features\n",
    "        self.edge_index = edge_indices\n",
    "        self.edge_attr = edge_features\n",
    "        self.variable_features = variable_features\n",
    "        self.candidates = candidates\n",
    "        self.nb_candidates = len(candidates)\n",
    "        self.candidate_choices = candidate_choice\n",
    "        self.candidate_scores = candidate_scores\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.constraint_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n",
    "\n",
    "\n",
    "class GraphDataset(torch_geometric.data.Dataset):\n",
    "    def __init__(self, sample_files):\n",
    "        super().__init__(root=None, transform=None, pre_transform=None)\n",
    "        self.sample_files = sample_files\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sample_files)\n",
    "\n",
    "    def get(self, index):\n",
    "        with gzip.open(self.sample_files[index], 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action, sample_action_set, sample_scores = sample\n",
    "\n",
    "        constraint_features, (edge_indices, edge_features), variable_features = sample_observation\n",
    "        constraint_features = torch.FloatTensor(constraint_features)\n",
    "        edge_indices = torch.LongTensor(edge_indices.astype(np.int32))\n",
    "        edge_features = torch.FloatTensor(np.expand_dims(edge_features, axis=-1))\n",
    "        variable_features = torch.FloatTensor(variable_features)\n",
    "\n",
    "        candidates = torch.LongTensor(np.array(sample_action_set, dtype=np.int32))\n",
    "        candidate_choice = torch.where(candidates == sample_action)[0][0]  # action index relative to candidates\n",
    "        candidate_scores = torch.FloatTensor([sample_scores[j] for j in candidates])\n",
    "\n",
    "        graph = BipartiteNodeData(constraint_features, edge_indices, edge_features, variable_features,\n",
    "                                  candidates, candidate_choice, candidate_scores)\n",
    "        graph.num_nodes = constraint_features.shape[0]+variable_features.shape[0]\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then prepare the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = [str(path) for path in Path('samples/').glob('sample_*.pkl')]\n",
    "train_files = sample_files[:int(0.8*len(sample_files))]\n",
    "valid_files = sample_files[int(0.8*len(sample_files)):]\n",
    "\n",
    "train_data = GraphDataset(train_files)\n",
    "train_loader = torch_geometric.data.DataLoader(train_data, 32, shuffle=True)\n",
    "valid_data = GraphDataset(valid_files)\n",
    "valid_loader = torch_geometric.data.DataLoader(valid_data, 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our graph neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNPolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        emb_size = 64\n",
    "        cons_nfeats = 5\n",
    "        edge_nfeats = 1\n",
    "        var_nfeats = 19\n",
    "\n",
    "        # CONSTRAINT EMBEDDING\n",
    "        self.cons_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(cons_nfeats),\n",
    "            torch.nn.Linear(cons_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # EDGE EMBEDDING\n",
    "        self.edge_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(edge_nfeats),\n",
    "        )\n",
    "\n",
    "        # VARIABLE EMBEDDING\n",
    "        self.var_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(var_nfeats),\n",
    "            torch.nn.Linear(var_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_v_to_c = BipartiteGraphConvolution()\n",
    "        self.conv_c_to_v = BipartiteGraphConvolution()\n",
    "\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, constraint_features, edge_indices, edge_features, variable_features):\n",
    "        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n",
    "        \n",
    "        constraint_features = self.cons_embedding(constraint_features)\n",
    "        edge_features = self.edge_embedding(edge_features)\n",
    "        variable_features = self.var_embedding(variable_features)\n",
    "\n",
    "        constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, edge_features, constraint_features)\n",
    "        variable_features = self.conv_c_to_v(constraint_features, edge_indices, edge_features, variable_features)\n",
    "\n",
    "        output = self.output_module(variable_features).squeeze(-1)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__('add')\n",
    "        emb_size = 64\n",
    "        \n",
    "        self.feature_module_left = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        self.feature_module_edge = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_right = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_final = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        \n",
    "        self.post_conv_module = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size)\n",
    "        )\n",
    "\n",
    "        # output_layers\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, left_features, edge_indices, edge_features, right_features):\n",
    "        output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n",
    "                                node_features=(left_features, right_features), edge_features=edge_features)\n",
    "        return self.output_module(torch.cat([self.post_conv_module(output), right_features], dim=-1))\n",
    "\n",
    "    def message(self, node_features_i, node_features_j, edge_features):\n",
    "        output = self.feature_module_final(self.feature_module_left(node_features_i) \n",
    "                                           + self.feature_module_edge(edge_features) \n",
    "                                           + self.feature_module_right(node_features_j))\n",
    "        return output\n",
    "    \n",
    "\n",
    "policy = GNNPolicy().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we can predict a probability distribution over actions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0105, 0.0104, 0.0105, 0.0104, 0.0104, 0.0105, 0.0105, 0.0105, 0.0104,\n",
      "        0.0105, 0.0105, 0.0104, 0.0105, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104,\n",
      "        0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104,\n",
      "        0.0105, 0.0105, 0.0105, 0.0104, 0.0104, 0.0105, 0.0104, 0.0105, 0.0105,\n",
      "        0.0105, 0.0104, 0.0104, 0.0105, 0.0105, 0.0105, 0.0105, 0.0104, 0.0104,\n",
      "        0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0105, 0.0105, 0.0105, 0.0105,\n",
      "        0.0105, 0.0104, 0.0104, 0.0105, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104,\n",
      "        0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0105, 0.0104, 0.0105,\n",
      "        0.0104, 0.0105, 0.0104, 0.0105, 0.0103, 0.0104, 0.0104, 0.0104, 0.0104,\n",
      "        0.0104, 0.0105, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
      "        0.0104, 0.0105, 0.0104, 0.0105, 0.0104, 0.0105], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "observation = train_data[0].to(DEVICE)\n",
    "\n",
    "logits = policy(observation.constraint_features, observation.edge_index, observation.edge_attr, observation.variable_features)\n",
    "action_distribution = F.softmax(logits[observation.candidates], dim=-1)\n",
    "\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, with randomly initialized weights, the initial distributions tend to be close to uniform.\n",
    "Next, we will define two helper functions: one to train or evaluate the model on a whole epoch and compute metrics for monitoring, and one for padding tensors when doing predictions on a batch of graphs of potentially different number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(policy, data_loader, optimizer=None):\n",
    "    mean_loss = 0\n",
    "    mean_kacc = np.zeros(len([1, 3, 5, 10]))\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            logits = policy(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "            loss = F.cross_entropy(logits, batch.candidate_choices)\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            true_scores = pad_tensor(batch.candidate_scores, batch.nb_candidates)\n",
    "            true_bestscore = true_scores.max(dim=-1, keepdims=True).values\n",
    "\n",
    "            kacc = []\n",
    "            for k in [1, 3, 5, 10]:\n",
    "                if logits.size()[-1] < k:\n",
    "                    kacc.append(1.0)\n",
    "                    continue\n",
    "                pred_top_k = logits.topk(k).indices\n",
    "                pred_top_k_true_scores = true_scores.gather(-1, pred_top_k)\n",
    "                accuracy = (pred_top_k_true_scores == true_bestscore).any(dim=-1).float().mean().item()\n",
    "                kacc.append(accuracy)\n",
    "            kacc = np.asarray(kacc)\n",
    "\n",
    "            mean_loss += loss.item() * batch.num_graphs\n",
    "            mean_kacc += kacc * batch.num_graphs\n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_kacc /= n_samples_processed\n",
    "    return mean_loss, mean_kacc\n",
    "\n",
    "\n",
    "def pad_tensor(input_, pad_sizes, pad_value=-1e8):\n",
    "    max_pad_size = pad_sizes.max()\n",
    "    output = input_.split(pad_sizes.cpu().numpy().tolist())\n",
    "    output = torch.stack([F.pad(slice_, (0, max_pad_size-slice_.size(0)), 'constant', pad_value)\n",
    "                          for slice_ in output], dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can actually create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 4.288  acc@1: 0.265 acc@3: 0.424 acc@5: 0.495 acc@10: 0.620\n",
      "VALID LOSS: 3.957  acc@1: 0.285 acc@3: 0.485 acc@5: 0.560 acc@10: 0.645\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.701  acc@1: 0.411 acc@3: 0.569 acc@5: 0.635 acc@10: 0.728\n",
      "VALID LOSS: 3.524  acc@1: 0.490 acc@3: 0.615 acc@5: 0.700 acc@10: 0.810\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.483  acc@1: 0.472 acc@3: 0.615 acc@5: 0.691 acc@10: 0.814\n",
      "VALID LOSS: 3.581  acc@1: 0.455 acc@3: 0.580 acc@5: 0.660 acc@10: 0.805\n",
      "TRAIN LOSS: 3.486  acc@1: 0.456 acc@3: 0.619 acc@5: 0.699 acc@10: 0.810\n",
      "VALID LOSS: 3.496  acc@1: 0.485 acc@3: 0.590 acc@5: 0.690 acc@10: 0.800\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.465  acc@1: 0.465 acc@3: 0.626 acc@5: 0.703 acc@10: 0.820\n",
      "VALID LOSS: 3.520  acc@1: 0.465 acc@3: 0.620 acc@5: 0.695 acc@10: 0.795\n",
      "TRAIN LOSS: 3.456  acc@1: 0.466 acc@3: 0.625 acc@5: 0.698 acc@10: 0.810\n",
      "VALID LOSS: 3.452  acc@1: 0.485 acc@3: 0.620 acc@5: 0.690 acc@10: 0.820\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.465  acc@1: 0.474 acc@3: 0.623 acc@5: 0.696 acc@10: 0.820\n",
      "VALID LOSS: 3.451  acc@1: 0.495 acc@3: 0.615 acc@5: 0.680 acc@10: 0.830\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.462  acc@1: 0.474 acc@3: 0.631 acc@5: 0.708 acc@10: 0.811\n",
      "VALID LOSS: 3.465  acc@1: 0.450 acc@3: 0.600 acc@5: 0.700 acc@10: 0.810\n",
      "TRAIN LOSS: 3.447  acc@1: 0.466 acc@3: 0.636 acc@5: 0.708 acc@10: 0.836\n",
      "VALID LOSS: 3.428  acc@1: 0.465 acc@3: 0.580 acc@5: 0.680 acc@10: 0.810\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.458  acc@1: 0.466 acc@3: 0.627 acc@5: 0.708 acc@10: 0.821\n",
      "VALID LOSS: 3.546  acc@1: 0.480 acc@3: 0.625 acc@5: 0.690 acc@10: 0.795\n",
      "TRAIN LOSS: 3.451  acc@1: 0.466 acc@3: 0.632 acc@5: 0.698 acc@10: 0.825\n",
      "VALID LOSS: 3.473  acc@1: 0.470 acc@3: 0.610 acc@5: 0.680 acc@10: 0.815\n",
      "TRAIN LOSS: 3.433  acc@1: 0.470 acc@3: 0.631 acc@5: 0.711 acc@10: 0.824\n",
      "VALID LOSS: 3.420  acc@1: 0.460 acc@3: 0.585 acc@5: 0.670 acc@10: 0.815\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.441  acc@1: 0.461 acc@3: 0.626 acc@5: 0.700 acc@10: 0.818\n",
      "VALID LOSS: 3.557  acc@1: 0.450 acc@3: 0.615 acc@5: 0.700 acc@10: 0.800\n",
      "TRAIN LOSS: 3.422  acc@1: 0.459 acc@3: 0.629 acc@5: 0.708 acc@10: 0.834\n",
      "VALID LOSS: 3.464  acc@1: 0.465 acc@3: 0.610 acc@5: 0.670 acc@10: 0.835\n",
      "TRAIN LOSS: 3.452  acc@1: 0.461 acc@3: 0.625 acc@5: 0.696 acc@10: 0.833\n",
      "VALID LOSS: 3.492  acc@1: 0.485 acc@3: 0.620 acc@5: 0.710 acc@10: 0.815\n",
      "TRAIN LOSS: 3.442  acc@1: 0.472 acc@3: 0.636 acc@5: 0.710 acc@10: 0.825\n",
      "VALID LOSS: 3.445  acc@1: 0.485 acc@3: 0.600 acc@5: 0.675 acc@10: 0.810\n",
      "TRAIN LOSS: 3.407  acc@1: 0.479 acc@3: 0.637 acc@5: 0.719 acc@10: 0.830\n",
      "VALID LOSS: 3.444  acc@1: 0.480 acc@3: 0.615 acc@5: 0.700 acc@10: 0.820\n",
      "TRAIN LOSS: 3.427  acc@1: 0.469 acc@3: 0.626 acc@5: 0.716 acc@10: 0.838\n",
      "VALID LOSS: 3.458  acc@1: 0.475 acc@3: 0.615 acc@5: 0.690 acc@10: 0.825\n",
      "TRAIN LOSS: 3.387  acc@1: 0.479 acc@3: 0.639 acc@5: 0.724 acc@10: 0.840\n",
      "VALID LOSS: 3.434  acc@1: 0.470 acc@3: 0.630 acc@5: 0.695 acc@10: 0.830\n",
      "TRAIN LOSS: 3.339  acc@1: 0.469 acc@3: 0.650 acc@5: 0.734 acc@10: 0.841\n",
      "VALID LOSS: 3.451  acc@1: 0.465 acc@3: 0.625 acc@5: 0.695 acc@10: 0.795\n",
      "TRAIN LOSS: 3.349  acc@1: 0.484 acc@3: 0.647 acc@5: 0.734 acc@10: 0.843\n",
      "VALID LOSS: 3.431  acc@1: 0.455 acc@3: 0.620 acc@5: 0.685 acc@10: 0.820\n",
      "TRAIN LOSS: 3.376  acc@1: 0.465 acc@3: 0.635 acc@5: 0.720 acc@10: 0.825\n",
      "VALID LOSS: 3.524  acc@1: 0.470 acc@3: 0.605 acc@5: 0.695 acc@10: 0.820\n",
      "  10 epochs without improvement, decreasing learning rate to 0.0002\n",
      "TRAIN LOSS: 3.362  acc@1: 0.482 acc@3: 0.654 acc@5: 0.721 acc@10: 0.848\n",
      "VALID LOSS: 3.431  acc@1: 0.490 acc@3: 0.625 acc@5: 0.700 acc@10: 0.810\n",
      "TRAIN LOSS: 3.297  acc@1: 0.494 acc@3: 0.647 acc@5: 0.728 acc@10: 0.840\n",
      "VALID LOSS: 3.416  acc@1: 0.485 acc@3: 0.620 acc@5: 0.680 acc@10: 0.815\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.279  acc@1: 0.481 acc@3: 0.651 acc@5: 0.736 acc@10: 0.853\n",
      "VALID LOSS: 3.411  acc@1: 0.495 acc@3: 0.610 acc@5: 0.700 acc@10: 0.830\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.268  acc@1: 0.481 acc@3: 0.651 acc@5: 0.743 acc@10: 0.848\n",
      "VALID LOSS: 3.433  acc@1: 0.500 acc@3: 0.615 acc@5: 0.700 acc@10: 0.820\n",
      "TRAIN LOSS: 3.260  acc@1: 0.477 acc@3: 0.637 acc@5: 0.736 acc@10: 0.845\n",
      "VALID LOSS: 3.449  acc@1: 0.490 acc@3: 0.620 acc@5: 0.695 acc@10: 0.820\n",
      "TRAIN LOSS: 3.260  acc@1: 0.484 acc@3: 0.654 acc@5: 0.744 acc@10: 0.840\n",
      "VALID LOSS: 3.397  acc@1: 0.490 acc@3: 0.620 acc@5: 0.690 acc@10: 0.830\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.242  acc@1: 0.480 acc@3: 0.659 acc@5: 0.740 acc@10: 0.844\n",
      "VALID LOSS: 3.446  acc@1: 0.485 acc@3: 0.615 acc@5: 0.690 acc@10: 0.835\n",
      "TRAIN LOSS: 3.264  acc@1: 0.480 acc@3: 0.646 acc@5: 0.734 acc@10: 0.843\n",
      "VALID LOSS: 3.412  acc@1: 0.475 acc@3: 0.610 acc@5: 0.695 acc@10: 0.820\n",
      "TRAIN LOSS: 3.244  acc@1: 0.477 acc@3: 0.657 acc@5: 0.746 acc@10: 0.850\n",
      "VALID LOSS: 3.408  acc@1: 0.455 acc@3: 0.620 acc@5: 0.665 acc@10: 0.795\n",
      "TRAIN LOSS: 3.250  acc@1: 0.463 acc@3: 0.644 acc@5: 0.726 acc@10: 0.846\n",
      "VALID LOSS: 3.434  acc@1: 0.490 acc@3: 0.605 acc@5: 0.700 acc@10: 0.800\n",
      "TRAIN LOSS: 3.240  acc@1: 0.476 acc@3: 0.652 acc@5: 0.738 acc@10: 0.850\n",
      "VALID LOSS: 3.437  acc@1: 0.470 acc@3: 0.625 acc@5: 0.700 acc@10: 0.820\n",
      "TRAIN LOSS: 3.224  acc@1: 0.476 acc@3: 0.657 acc@5: 0.740 acc@10: 0.853\n",
      "VALID LOSS: 3.414  acc@1: 0.495 acc@3: 0.605 acc@5: 0.700 acc@10: 0.815\n",
      "TRAIN LOSS: 3.223  acc@1: 0.475 acc@3: 0.652 acc@5: 0.746 acc@10: 0.841\n",
      "VALID LOSS: 3.414  acc@1: 0.490 acc@3: 0.620 acc@5: 0.705 acc@10: 0.825\n",
      "TRAIN LOSS: 3.219  acc@1: 0.469 acc@3: 0.660 acc@5: 0.738 acc@10: 0.840\n",
      "VALID LOSS: 3.462  acc@1: 0.465 acc@3: 0.630 acc@5: 0.715 acc@10: 0.820\n",
      "TRAIN LOSS: 3.197  acc@1: 0.469 acc@3: 0.649 acc@5: 0.743 acc@10: 0.844\n",
      "VALID LOSS: 3.399  acc@1: 0.470 acc@3: 0.610 acc@5: 0.685 acc@10: 0.815\n",
      "TRAIN LOSS: 3.199  acc@1: 0.464 acc@3: 0.655 acc@5: 0.746 acc@10: 0.850\n",
      "VALID LOSS: 3.420  acc@1: 0.455 acc@3: 0.605 acc@5: 0.695 acc@10: 0.820\n",
      "  10 epochs without improvement, decreasing learning rate to 4e-05\n",
      "TRAIN LOSS: 3.182  acc@1: 0.469 acc@3: 0.655 acc@5: 0.741 acc@10: 0.845\n",
      "VALID LOSS: 3.420  acc@1: 0.470 acc@3: 0.620 acc@5: 0.710 acc@10: 0.825\n",
      "TRAIN LOSS: 3.171  acc@1: 0.474 acc@3: 0.660 acc@5: 0.751 acc@10: 0.846\n",
      "VALID LOSS: 3.405  acc@1: 0.475 acc@3: 0.615 acc@5: 0.695 acc@10: 0.820\n",
      "TRAIN LOSS: 3.165  acc@1: 0.470 acc@3: 0.650 acc@5: 0.745 acc@10: 0.849\n",
      "VALID LOSS: 3.413  acc@1: 0.490 acc@3: 0.610 acc@5: 0.715 acc@10: 0.825\n",
      "TRAIN LOSS: 3.165  acc@1: 0.475 acc@3: 0.659 acc@5: 0.752 acc@10: 0.848\n",
      "VALID LOSS: 3.403  acc@1: 0.455 acc@3: 0.615 acc@5: 0.685 acc@10: 0.815\n",
      "TRAIN LOSS: 3.161  acc@1: 0.470 acc@3: 0.656 acc@5: 0.746 acc@10: 0.844\n",
      "VALID LOSS: 3.426  acc@1: 0.470 acc@3: 0.610 acc@5: 0.710 acc@10: 0.825\n",
      "TRAIN LOSS: 3.165  acc@1: 0.475 acc@3: 0.666 acc@5: 0.750 acc@10: 0.844\n",
      "VALID LOSS: 3.407  acc@1: 0.480 acc@3: 0.615 acc@5: 0.705 acc@10: 0.825\n",
      "TRAIN LOSS: 3.159  acc@1: 0.477 acc@3: 0.664 acc@5: 0.751 acc@10: 0.850\n",
      "VALID LOSS: 3.394  acc@1: 0.465 acc@3: 0.610 acc@5: 0.685 acc@10: 0.810\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.156  acc@1: 0.464 acc@3: 0.660 acc@5: 0.744 acc@10: 0.845\n",
      "VALID LOSS: 3.432  acc@1: 0.470 acc@3: 0.610 acc@5: 0.720 acc@10: 0.825\n",
      "TRAIN LOSS: 3.155  acc@1: 0.474 acc@3: 0.667 acc@5: 0.754 acc@10: 0.850\n",
      "VALID LOSS: 3.392  acc@1: 0.455 acc@3: 0.615 acc@5: 0.685 acc@10: 0.810\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.153  acc@1: 0.469 acc@3: 0.657 acc@5: 0.741 acc@10: 0.845\n",
      "VALID LOSS: 3.405  acc@1: 0.480 acc@3: 0.615 acc@5: 0.700 acc@10: 0.810\n",
      "TRAIN LOSS: 3.146  acc@1: 0.466 acc@3: 0.660 acc@5: 0.749 acc@10: 0.844\n",
      "VALID LOSS: 3.396  acc@1: 0.475 acc@3: 0.605 acc@5: 0.705 acc@10: 0.805\n",
      "TRAIN LOSS: 3.144  acc@1: 0.470 acc@3: 0.661 acc@5: 0.749 acc@10: 0.844\n",
      "VALID LOSS: 3.401  acc@1: 0.465 acc@3: 0.610 acc@5: 0.695 acc@10: 0.810\n",
      "TRAIN LOSS: 3.139  acc@1: 0.464 acc@3: 0.655 acc@5: 0.745 acc@10: 0.844\n",
      "VALID LOSS: 3.425  acc@1: 0.475 acc@3: 0.615 acc@5: 0.705 acc@10: 0.820\n",
      "TRAIN LOSS: 3.138  acc@1: 0.469 acc@3: 0.657 acc@5: 0.746 acc@10: 0.844\n",
      "VALID LOSS: 3.416  acc@1: 0.470 acc@3: 0.620 acc@5: 0.700 acc@10: 0.810\n",
      "TRAIN LOSS: 3.135  acc@1: 0.465 acc@3: 0.660 acc@5: 0.751 acc@10: 0.849\n",
      "VALID LOSS: 3.420  acc@1: 0.470 acc@3: 0.620 acc@5: 0.700 acc@10: 0.810\n",
      "TRAIN LOSS: 3.132  acc@1: 0.475 acc@3: 0.662 acc@5: 0.749 acc@10: 0.853\n",
      "VALID LOSS: 3.394  acc@1: 0.470 acc@3: 0.605 acc@5: 0.690 acc@10: 0.815\n",
      "TRAIN LOSS: 3.134  acc@1: 0.463 acc@3: 0.656 acc@5: 0.743 acc@10: 0.854\n",
      "VALID LOSS: 3.399  acc@1: 0.470 acc@3: 0.615 acc@5: 0.705 acc@10: 0.815\n",
      "TRAIN LOSS: 3.126  acc@1: 0.465 acc@3: 0.659 acc@5: 0.749 acc@10: 0.849\n",
      "VALID LOSS: 3.415  acc@1: 0.465 acc@3: 0.620 acc@5: 0.695 acc@10: 0.820\n",
      "TRAIN LOSS: 3.128  acc@1: 0.470 acc@3: 0.664 acc@5: 0.750 acc@10: 0.853\n",
      "VALID LOSS: 3.405  acc@1: 0.470 acc@3: 0.610 acc@5: 0.700 acc@10: 0.815\n",
      "  10 epochs without improvement, decreasing learning rate to 8.000000000000001e-06\n",
      "TRAIN LOSS: 3.118  acc@1: 0.465 acc@3: 0.660 acc@5: 0.748 acc@10: 0.849\n",
      "VALID LOSS: 3.406  acc@1: 0.470 acc@3: 0.610 acc@5: 0.705 acc@10: 0.815\n",
      "TRAIN LOSS: 3.118  acc@1: 0.464 acc@3: 0.660 acc@5: 0.749 acc@10: 0.850\n",
      "VALID LOSS: 3.403  acc@1: 0.470 acc@3: 0.615 acc@5: 0.705 acc@10: 0.810\n",
      "TRAIN LOSS: 3.117  acc@1: 0.465 acc@3: 0.664 acc@5: 0.751 acc@10: 0.854\n",
      "VALID LOSS: 3.408  acc@1: 0.470 acc@3: 0.610 acc@5: 0.700 acc@10: 0.815\n",
      "TRAIN LOSS: 3.115  acc@1: 0.465 acc@3: 0.661 acc@5: 0.749 acc@10: 0.850\n",
      "VALID LOSS: 3.405  acc@1: 0.465 acc@3: 0.615 acc@5: 0.705 acc@10: 0.815\n",
      "TRAIN LOSS: 3.115  acc@1: 0.464 acc@3: 0.664 acc@5: 0.750 acc@10: 0.853\n",
      "VALID LOSS: 3.406  acc@1: 0.465 acc@3: 0.610 acc@5: 0.695 acc@10: 0.815\n",
      "TRAIN LOSS: 3.114  acc@1: 0.463 acc@3: 0.662 acc@5: 0.746 acc@10: 0.851\n",
      "VALID LOSS: 3.406  acc@1: 0.470 acc@3: 0.615 acc@5: 0.705 acc@10: 0.815\n",
      "TRAIN LOSS: 3.113  acc@1: 0.463 acc@3: 0.662 acc@5: 0.748 acc@10: 0.850\n",
      "VALID LOSS: 3.413  acc@1: 0.465 acc@3: 0.610 acc@5: 0.700 acc@10: 0.815\n",
      "TRAIN LOSS: 3.113  acc@1: 0.465 acc@3: 0.664 acc@5: 0.749 acc@10: 0.853\n",
      "VALID LOSS: 3.404  acc@1: 0.470 acc@3: 0.610 acc@5: 0.700 acc@10: 0.815\n",
      "TRAIN LOSS: 3.112  acc@1: 0.463 acc@3: 0.662 acc@5: 0.748 acc@10: 0.853\n",
      "VALID LOSS: 3.411  acc@1: 0.460 acc@3: 0.610 acc@5: 0.695 acc@10: 0.815\n",
      "TRAIN LOSS: 3.112  acc@1: 0.466 acc@3: 0.659 acc@5: 0.749 acc@10: 0.851\n",
      "VALID LOSS: 3.410  acc@1: 0.460 acc@3: 0.610 acc@5: 0.695 acc@10: 0.815\n",
      "  20 epochs without improvement, early stopping\n",
      "BEST VALID LOSS: 3.392  acc@1: 0.455 acc@3: 0.615 acc@5: 0.685 acc@10: 0.810\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = np.inf\n",
    "for epoch in range(MAX_EPOCHS + 1):\n",
    "    train_loss, train_kacc = process(policy, train_loader, optimizer)\n",
    "    print(f\"TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], train_kacc)]))\n",
    "\n",
    "    valid_loss, valid_kacc = process(policy, valid_loader, None)\n",
    "    print(f\"VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], valid_kacc)]))\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        plateau_count = 0\n",
    "        best_loss = valid_loss\n",
    "        torch.save(policy.state_dict(), 'trained_params.pkl')\n",
    "        print(f\"  best model so far\")\n",
    "    else:\n",
    "        plateau_count += 1\n",
    "        if plateau_count % EARLY_STOPPING == 0:\n",
    "            print(f\"  {plateau_count} epochs without improvement, early stopping\")\n",
    "            break\n",
    "        if plateau_count % PATIENCE == 0:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.2\n",
    "            print(f\"  {plateau_count} epochs without improvement, decreasing learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        \n",
    "policy.load_state_dict(torch.load('trained_params.pkl'))\n",
    "valid_loss, valid_kacc = process(policy, valid_loader, None)\n",
    "print(f\"BEST VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], valid_kacc)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluation\n",
    "\n",
    "Finally, we can evaluate the performance of the model. We first define appropriate environments. For benchmarking purposes, we include a trivial environment that merely runs SCIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "env = ecole.environment.Branching(observation_function=ecole.observation.NodeBipartite(), \n",
    "                                  information_function={\"nb_nodes\": ecole.reward.NNodes(), \"time\": ecole.reward.SolvingTime()}, \n",
    "                                  scip_params=scip_parameters)\n",
    "default_env = ecole.environment.Configuring(observation_function=None,\n",
    "                                            information_function={\"nb_nodes\": ecole.reward.NNodes(), \"time\": ecole.reward.SolvingTime()}, \n",
    "                                            scip_params=scip_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply follow the environments, taking steps appropriately according to the GNN policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes  956 | GNN nb nodes 1294 | Gain   -35.36%\n",
      "             | SCIP time    10.27 | GNN time     6.40 | Gain    37.68%\n",
      "Instance   1 | SCIP nb nodes   88 | GNN nb nodes  308 | Gain  -250.00%\n",
      "             | SCIP time     5.64 | GNN time     1.74 | Gain    69.15%\n",
      "Instance   2 | SCIP nb nodes   37 | GNN nb nodes  270 | Gain  -629.73%\n",
      "             | SCIP time     5.17 | GNN time     1.34 | Gain    74.08%\n",
      "Instance   3 | SCIP nb nodes  145 | GNN nb nodes  268 | Gain   -84.83%\n",
      "             | SCIP time     6.55 | GNN time     1.55 | Gain    76.34%\n",
      "Instance   4 | SCIP nb nodes   34 | GNN nb nodes  248 | Gain  -629.41%\n",
      "             | SCIP time     4.30 | GNN time     1.14 | Gain    73.49%\n",
      "Instance   5 | SCIP nb nodes   17 | GNN nb nodes  110 | Gain  -547.06%\n",
      "             | SCIP time     4.39 | GNN time     0.59 | Gain    86.56%\n",
      "Instance   6 | SCIP nb nodes   69 | GNN nb nodes  196 | Gain  -184.06%\n",
      "             | SCIP time     5.22 | GNN time     1.18 | Gain    77.39%\n",
      "Instance   7 | SCIP nb nodes  991 | GNN nb nodes 3632 | Gain  -266.50%\n",
      "             | SCIP time    11.92 | GNN time    18.79 | Gain   -57.63%\n",
      "Instance   8 | SCIP nb nodes   11 | GNN nb nodes  124 | Gain -1027.27%\n",
      "             | SCIP time     3.36 | GNN time     0.73 | Gain    78.27%\n",
      "Instance   9 | SCIP nb nodes   41 | GNN nb nodes  240 | Gain  -485.37%\n",
      "             | SCIP time     4.20 | GNN time     1.28 | Gain    69.52%\n",
      "Instance  10 | SCIP nb nodes   31 | GNN nb nodes  434 | Gain -1300.00%\n",
      "             | SCIP time     5.88 | GNN time     2.37 | Gain    59.69%\n",
      "Instance  11 | SCIP nb nodes   17 | GNN nb nodes   84 | Gain  -394.12%\n",
      "             | SCIP time     4.17 | GNN time     0.53 | Gain    87.29%\n",
      "Instance  12 | SCIP nb nodes   19 | GNN nb nodes  144 | Gain  -657.89%\n",
      "             | SCIP time     4.39 | GNN time     0.84 | Gain    80.87%\n",
      "Instance  13 | SCIP nb nodes  111 | GNN nb nodes  328 | Gain  -195.50%\n",
      "             | SCIP time     6.12 | GNN time     1.63 | Gain    73.37%\n",
      "Instance  14 | SCIP nb nodes  103 | GNN nb nodes  574 | Gain  -457.28%\n",
      "             | SCIP time     5.68 | GNN time     2.62 | Gain    53.87%\n",
      "Instance  15 | SCIP nb nodes   27 | GNN nb nodes  110 | Gain  -307.41%\n",
      "             | SCIP time     3.53 | GNN time     0.63 | Gain    82.15%\n",
      "Instance  16 | SCIP nb nodes   13 | GNN nb nodes  182 | Gain -1300.00%\n",
      "             | SCIP time     3.80 | GNN time     1.08 | Gain    71.58%\n",
      "Instance  17 | SCIP nb nodes   17 | GNN nb nodes  142 | Gain  -735.29%\n",
      "             | SCIP time     4.57 | GNN time     0.91 | Gain    80.09%\n",
      "Instance  18 | SCIP nb nodes    9 | GNN nb nodes   88 | Gain  -877.78%\n",
      "             | SCIP time     3.58 | GNN time     0.49 | Gain    86.31%\n",
      "Instance  19 | SCIP nb nodes    7 | GNN nb nodes   70 | Gain  -900.00%\n",
      "             | SCIP time     2.40 | GNN time     0.36 | Gain    85.00%\n"
     ]
    }
   ],
   "source": [
    "for instance_count, instance in zip(range(20), instances):\n",
    "    nb_nodes, time = 0, 0\n",
    "\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                           torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE), \n",
    "                           torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                           torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE))\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "        nb_nodes += info['nb_nodes']\n",
    "        time += info['time']\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance.copy_orig())\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "\n",
    "    print(f\"Instance {instance_count: >3} | SCIP nb nodes {int(default_info['nb_nodes']): >4d} \"\n",
    "          f\"| GNN nb nodes {int(nb_nodes): >4d} | Gain {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}%\")\n",
    "    print(f\"             | SCIP time   {default_info['time']: >6.2f} \"\n",
    "          f\"| GNN time   {time: >6.2f} | Gain {100*(1-time/default_info['time']): >8.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate on instances larger and harder than those trained on, say with 600 rather than 500 contraints.\n",
    "In addition, we showcase that the cumulative number of nodes and time required to solve an instance can also be computed directly using the `.cumsum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance   0 | SCIP nb nodes   93 | GNN nb nodes  502 | Gain  -439.78%\n",
      "             | SCIP time     5.07 | GNN time     4.72 | Gain     6.90%\n",
      "Instance   1 | SCIP nb nodes 1601 | GNN nb nodes 3109 | Gain   -94.19%\n",
      "             | SCIP time    14.64 | GNN time    20.19 | Gain   -37.91%\n",
      "Instance   2 | SCIP nb nodes  361 | GNN nb nodes  878 | Gain  -143.21%\n",
      "             | SCIP time     9.74 | GNN time     8.84 | Gain     9.24%\n",
      "Instance   3 | SCIP nb nodes    5 | GNN nb nodes   29 | Gain  -480.00%\n",
      "             | SCIP time     4.29 | GNN time     3.07 | Gain    28.44%\n",
      "Instance   4 | SCIP nb nodes  641 | GNN nb nodes 1616 | Gain  -152.11%\n",
      "             | SCIP time    11.48 | GNN time    12.78 | Gain   -11.32%\n",
      "Instance   5 | SCIP nb nodes   11 | GNN nb nodes  151 | Gain -1272.73%\n",
      "             | SCIP time     5.29 | GNN time     4.03 | Gain    23.82%\n",
      "Instance   6 | SCIP nb nodes    5 | GNN nb nodes   47 | Gain  -840.00%\n",
      "             | SCIP time     3.29 | GNN time     2.63 | Gain    20.06%\n",
      "Instance   7 | SCIP nb nodes   61 | GNN nb nodes  361 | Gain  -491.80%\n",
      "             | SCIP time     6.35 | GNN time     5.10 | Gain    19.69%\n",
      "Instance   8 | SCIP nb nodes   15 | GNN nb nodes  101 | Gain  -573.33%\n",
      "             | SCIP time     4.61 | GNN time     3.58 | Gain    22.34%\n",
      "Instance   9 | SCIP nb nodes   13 | GNN nb nodes   95 | Gain  -630.77%\n",
      "             | SCIP time     4.45 | GNN time     3.32 | Gain    25.39%\n",
      "Instance  10 | SCIP nb nodes  464 | GNN nb nodes 1208 | Gain  -160.34%\n",
      "             | SCIP time    12.64 | GNN time    12.17 | Gain     3.72%\n",
      "Instance  11 | SCIP nb nodes 1303 | GNN nb nodes 3581 | Gain  -174.83%\n",
      "             | SCIP time    17.22 | GNN time    28.31 | Gain   -64.40%\n",
      "Instance  12 | SCIP nb nodes   15 | GNN nb nodes   41 | Gain  -173.33%\n",
      "             | SCIP time     3.25 | GNN time     2.28 | Gain    29.85%\n",
      "Instance  13 | SCIP nb nodes  131 | GNN nb nodes  576 | Gain  -339.69%\n",
      "             | SCIP time     7.01 | GNN time     6.38 | Gain     8.99%\n",
      "Instance  14 | SCIP nb nodes 2947 | GNN nb nodes 7383 | Gain  -150.53%\n",
      "             | SCIP time    25.58 | GNN time    50.50 | Gain   -97.42%\n",
      "Instance  15 | SCIP nb nodes  319 | GNN nb nodes 1047 | Gain  -228.21%\n",
      "             | SCIP time    10.27 | GNN time    10.85 | Gain    -5.65%\n",
      "Instance  16 | SCIP nb nodes   95 | GNN nb nodes  293 | Gain  -208.42%\n",
      "             | SCIP time     5.52 | GNN time     4.35 | Gain    21.20%\n",
      "Instance  17 | SCIP nb nodes  105 | GNN nb nodes  703 | Gain  -569.52%\n",
      "             | SCIP time     7.17 | GNN time     6.64 | Gain     7.39%\n",
      "Instance  18 | SCIP nb nodes   76 | GNN nb nodes  203 | Gain  -167.11%\n",
      "             | SCIP time     7.33 | GNN time     4.97 | Gain    32.20%\n",
      "Instance  19 | SCIP nb nodes  380 | GNN nb nodes 1257 | Gain  -230.79%\n",
      "             | SCIP time     8.82 | GNN time     9.81 | Gain   -11.22%\n"
     ]
    }
   ],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=600, n_cols=1000, density=0.05)\n",
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600}\n",
    "env = ecole.environment.Branching(observation_function=ecole.observation.NodeBipartite(), \n",
    "                                  information_function={\"nb_nodes\": ecole.reward.NNodes().cumsum(), \n",
    "                                                        \"time\": ecole.reward.SolvingTime().cumsum()}, \n",
    "                                  scip_params=scip_parameters)\n",
    "default_env = ecole.environment.Configuring(observation_function=None,\n",
    "                                            information_function={\"nb_nodes\": ecole.reward.NNodes().cumsum(), \n",
    "                                                                  \"time\": ecole.reward.SolvingTime().cumsum()}, \n",
    "                                            scip_params=scip_parameters)\n",
    "\n",
    "for instance_count, instance in zip(range(20), instances):\n",
    "    observation, action_set, _, done, info = env.reset(instance)\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            observation = (torch.from_numpy(observation.row_features.astype(np.float32)).to(DEVICE),\n",
    "                           torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(DEVICE), \n",
    "                           torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(DEVICE),\n",
    "                           torch.from_numpy(observation.column_features.astype(np.float32)).to(DEVICE))\n",
    "            logits = policy(*observation)\n",
    "            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "            observation, action_set, _, done, info = env.step(action)\n",
    "    nb_nodes = info['nb_nodes']\n",
    "    time = info['time']\n",
    "\n",
    "    # Run SCIP's default brancher\n",
    "    default_env.reset(instance.copy_orig())\n",
    "    _, _, _, _, default_info = default_env.step({})\n",
    "\n",
    "    print(f\"Instance {instance_count: >3} | SCIP nb nodes {int(default_info['nb_nodes']): >4d} \"\n",
    "          f\"| GNN nb nodes {int(nb_nodes): >4d} | Gain {100*(1-nb_nodes/default_info['nb_nodes']): >8.2f}%\")\n",
    "    print(f\"             | SCIP time   {default_info['time']: >6.2f} \"\n",
    "          f\"| GNN time   {time: >6.2f} | Gain {100*(1-time/default_info['time']): >8.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
