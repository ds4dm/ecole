{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching with Imitation Learning and a GNN\n",
    "\n",
    "In this tutorial we will reproduce the Gasse et al. (2019) paper on learning to branch with Ecole in Tensorflow 2. We collect strong branching examples on randomly generated Combinatorial Auctions instances, then train a graph neural network with bipartite state encodings to imitate the expert by classification. Finally, we will evaluate the quality of the policy.\n",
    "\n",
    "To avoid burdening the code too much, some simplifications were made to the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "Our first step will be to run explore-then-strong-branch on randomly generated Combinatorial Auctions instances, and save the branching decisions to build a dataset. We will also record the state of the branch-and-bound process as a bipartite graph, which is already implemented in Ecole with the same features as Gasse et al. (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ecole\n",
    "from utilities import InstanceGenerator, generate_cauctions\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate Combinatorial Auctions instances on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = InstanceGenerator(generate_cauctions, n_items=100, n_bids=100, add_item_prob=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explore-then-strong-branch scheme described in the paper is not implemented by default in Ecole, but we can easily write this branching rule in python, which showcasees the flexibility of Ecole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreThenStrongBranch:\n",
    "    def __init__(self, expert_probability):\n",
    "        self.expert_probability = expert_probability\n",
    "        self.pseudocosts_function = ecole.observation.Pseudocosts()\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "    \n",
    "    def reset(self, model):\n",
    "        self.pseudocosts_function.reset(model)\n",
    "        self.strong_branching_function.reset(model)\n",
    "    \n",
    "    def obtain_observation(self, model):\n",
    "        probabilities = [1-self.expert_probability, self.expert_probability]\n",
    "        expert_chosen = bool(np.random.choice(np.arange(2), p=probabilities))\n",
    "        if expert_chosen:\n",
    "            return (self.strong_branching_function.obtain_observation(model), True)\n",
    "        else:\n",
    "            return (self.pseudocosts_function.obtain_observation(model), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the environment with the correct parameters (no restarts, 1h time limit, 5% expert sampling probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': 3600, 'timing/clocktype': 2}\n",
    "observation_function = ecole.observation.TupleFunction(ExploreThenStrongBranch(expert_probability=0.05), \n",
    "                                                       ecole.observation.NodeBipartite())\n",
    "env = ecole.environment.Branching(observation_function=observation_function, scip_params=scip_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then loop over the instances, following the strong branching expert 5% of the time and saving its decision, until 10000 samples is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, 17 samples collected so far\n",
      "Episode 50, 39 samples collected so far\n",
      "Episode 75, 52 samples collected so far\n",
      "Episode 100, 76 samples collected so far\n",
      "Episode 125, 95 samples collected so far\n"
     ]
    }
   ],
   "source": [
    "episode_counter = 0\n",
    "sample_counter = 0\n",
    "max_samples_reached = False\n",
    "Path('samples/').mkdir(exist_ok=True)\n",
    "env.seed(0)\n",
    "\n",
    "while not max_samples_reached:\n",
    "    episode_counter += 1\n",
    "    with next(instances) as instance:\n",
    "        observation, action_set, _, done = env.reset(instance.name)\n",
    "    \n",
    "    while not done:\n",
    "        scores, node_observation = observation\n",
    "        scores, scores_are_expert = scores\n",
    "        node_observation = (node_observation.row_features,\n",
    "                            (node_observation.edge_features.indices, \n",
    "                             node_observation.edge_features.values),\n",
    "                            node_observation.column_features)\n",
    "\n",
    "        action = action_set[scores[action_set].argmax()]\n",
    "\n",
    "        if scores_are_expert and not max_samples_reached:\n",
    "            sample_counter += 1\n",
    "            data = [node_observation, action, action_set, scores]\n",
    "            filename = f'samples/sample_{sample_counter}.pkl'\n",
    "\n",
    "            with gzip.open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            if sample_counter == 100:\n",
    "                max_samples_reached = True\n",
    "\n",
    "        observation, action_set, _, done, _ = env.step(action)\n",
    "    \n",
    "    if episode_counter % 25 == 0:\n",
    "        print(f\"Episode {episode_counter}, {sample_counter} samples collected so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a GNN\n",
    "\n",
    "Our next step is to train a GNN classifier on these collected samples to predict similar choices to strong branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gnn import GCNPolicy as GNN\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_EPOCHS = 10\n",
    "PATIENCE = 10\n",
    "EARLY_STOPPING = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define a helper function that can batch a set of samples to feed to the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_gcnn(sample_files):\n",
    "    \"\"\"\n",
    "    Loads and concatenates a bunch of samples into one mini-batch.\n",
    "    \"\"\"\n",
    "    c_features, e_indices, e_features, v_features = [], [], [], []\n",
    "    candss, cand_choices, cand_scoress = [], [], []\n",
    "\n",
    "    # load samples\n",
    "    for filename in sample_files:\n",
    "        with gzip.open(filename.numpy(), 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action, sample_action_set, sample_scores = sample\n",
    "\n",
    "        sample_action_set = np.array(sample_action_set)\n",
    "        cand_choice = np.where(sample_action_set == sample_action)[0][0]  # action index relative to candidates\n",
    "        cand_scores = sample_scores[sample_action_set]\n",
    "\n",
    "        c, (ei, ev), v = sample_observation\n",
    "        c_features.append(c)\n",
    "        e_indices.append(ei)\n",
    "        e_features.append(tf.expand_dims(ev, -1))\n",
    "        v_features.append(v)\n",
    "        candss.append(sample_action_set)\n",
    "        cand_choices.append(cand_choice)\n",
    "        cand_scoress.append(cand_scores)\n",
    "\n",
    "    n_cs_per_sample = [c.shape[0] for c in c_features]\n",
    "    n_vs_per_sample = [v.shape[0] for v in v_features]\n",
    "    n_cands_per_sample = [cds.shape[0] for cds in candss]\n",
    "\n",
    "    # concatenate samples in one big graph\n",
    "    c_features = np.concatenate(c_features, axis=0)\n",
    "    v_features = np.concatenate(v_features, axis=0)\n",
    "    e_features = np.concatenate(e_features, axis=0)\n",
    "    # edge indices have to be adjusted accordingly\n",
    "    cv_shift = np.cumsum([[0] + n_cs_per_sample[:-1],\n",
    "                          [0] + n_vs_per_sample[:-1]], axis=1)\n",
    "    e_indices = np.concatenate([e_ind + cv_shift[:, j:(j+1)]\n",
    "        for j, e_ind in enumerate(e_indices)], axis=1)\n",
    "    # candidate indices as well\n",
    "    candss = np.concatenate([cands + shift\n",
    "        for cands, shift in zip(candss, cv_shift[1])])\n",
    "    cand_choices = np.array(cand_choices)\n",
    "    cand_scoress = np.concatenate(cand_scoress, axis=0)\n",
    "\n",
    "    # convert to tensors\n",
    "    c_features = tf.convert_to_tensor(c_features, dtype=tf.float32)\n",
    "    e_indices = tf.convert_to_tensor(e_indices, dtype=tf.int32)\n",
    "    e_features = tf.convert_to_tensor(e_features, dtype=tf.float32)\n",
    "    v_features = tf.convert_to_tensor(v_features, dtype=tf.float32)\n",
    "    n_cs_per_sample = tf.convert_to_tensor(n_cs_per_sample, dtype=tf.int32)\n",
    "    n_vs_per_sample = tf.convert_to_tensor(n_vs_per_sample, dtype=tf.int32)\n",
    "    candss = tf.convert_to_tensor(candss, dtype=tf.int32)\n",
    "    cand_choices = tf.convert_to_tensor(cand_choices, dtype=tf.int32)\n",
    "    cand_scoress = tf.convert_to_tensor(cand_scoress, dtype=tf.float32)\n",
    "    n_cands_per_sample = tf.convert_to_tensor(n_cands_per_sample, dtype=tf.int32)\n",
    "\n",
    "    return c_features, e_indices, e_features, v_features, n_cs_per_sample, n_vs_per_sample, \\\n",
    "            n_cands_per_sample, candss, cand_choices, cand_scoress\n",
    "\n",
    "\n",
    "def load_batch_tf(x):\n",
    "    return tf.py_function(load_batch_gcnn, [x], [tf.float32, tf.int32, tf.float32, tf.float32, \n",
    "                                tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.float32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then prepare the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = [str(path) for path in Path('samples/').glob('sample_*.pkl')]\n",
    "train_files = sample_files[:int(0.7*len(sample_files))]\n",
    "valid_files = sample_files[int(0.7*len(sample_files)):int(0.85*len(sample_files))]\n",
    "test_files = sample_files[int(0.85*len(sample_files)):]\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "train_data = train_data.batch(32)\n",
    "train_data = train_data.map(load_batch_tf)\n",
    "train_data = train_data.prefetch(1)\n",
    "\n",
    "valid_data = tf.data.Dataset.from_tensor_slices(valid_files)\n",
    "valid_data = valid_data.batch(128)\n",
    "valid_data = valid_data.map(load_batch_tf)\n",
    "valid_data = valid_data.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a helper function to train or evaluate the model on a whole epoch, and compute metrics for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, dataloader, optimizer=None):\n",
    "    mean_loss = 0\n",
    "    mean_kacc = np.zeros(len([1, 3, 5, 10]))\n",
    "    \n",
    "    @tf.function(input_signature=model.input_signature)\n",
    "    def forward(inputs, training):\n",
    "        return model.call(inputs, training)\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    for batch in dataloader:\n",
    "        c, ei, ev, v, n_cs, n_vs, n_cands, cands, best_cands, cand_scores = batch\n",
    "        batched_states = (c, ei, ev, v, tf.reduce_sum(n_cs, keepdims=True), tf.reduce_sum(n_vs, keepdims=True))  # prevent padding\n",
    "        batch_size = len(n_cs.numpy())\n",
    "\n",
    "        if optimizer:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = forward(batched_states, tf.convert_to_tensor(True)) # training mode\n",
    "                logits = tf.expand_dims(tf.gather(tf.squeeze(logits, 0), cands), 0)  # filter candidate variables\n",
    "                logits = model.pad_output(logits, n_cands.numpy())  # apply padding now\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=best_cands, logits=logits)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(target=loss, sources=model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        else:\n",
    "            logits = forward(batched_states, tf.convert_to_tensor(False))  # eval mode\n",
    "            logits = tf.expand_dims(tf.gather(tf.squeeze(logits, 0), cands), 0)  # filter candidate variables\n",
    "            logits = model.pad_output(logits, n_cands.numpy())  # apply padding now\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=best_cands, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        true_scores = model.pad_output(tf.reshape(cand_scores, (1, -1)), n_cands)\n",
    "        true_bestscore = tf.reduce_max(true_scores, axis=-1, keepdims=True)\n",
    "        true_scores = true_scores.numpy()\n",
    "        true_bestscore = true_bestscore.numpy()\n",
    "\n",
    "        kacc = []\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            pred_top_k = tf.nn.top_k(logits, k=k)[1].numpy()\n",
    "            pred_top_k_true_scores = np.take_along_axis(true_scores, pred_top_k, axis=1)\n",
    "            kacc.append(np.mean(np.any(pred_top_k_true_scores == true_bestscore, axis=1)))\n",
    "        kacc = np.asarray(kacc)\n",
    "\n",
    "        mean_loss += loss.numpy() * batch_size\n",
    "        mean_kacc += kacc * batch_size\n",
    "        n_samples_processed += batch_size\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_kacc /= n_samples_processed\n",
    "\n",
    "    return mean_loss, mean_kacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can actually create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 3.613  acc@1: 0.243 acc@3: 0.471 acc@5: 0.557 acc@10: 0.714\n",
      "VALID LOSS: 3.491  acc@1: 0.133 acc@3: 0.333 acc@5: 0.467 acc@10: 0.667\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.532  acc@1: 0.229 acc@3: 0.357 acc@5: 0.486 acc@10: 0.743\n",
      "VALID LOSS: 3.453  acc@1: 0.133 acc@3: 0.333 acc@5: 0.400 acc@10: 0.667\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.442  acc@1: 0.214 acc@3: 0.329 acc@5: 0.514 acc@10: 0.743\n",
      "VALID LOSS: 3.403  acc@1: 0.133 acc@3: 0.333 acc@5: 0.400 acc@10: 0.667\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.329  acc@1: 0.214 acc@3: 0.357 acc@5: 0.529 acc@10: 0.743\n",
      "VALID LOSS: 3.352  acc@1: 0.200 acc@3: 0.333 acc@5: 0.400 acc@10: 0.667\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.202  acc@1: 0.243 acc@3: 0.386 acc@5: 0.571 acc@10: 0.757\n",
      "VALID LOSS: 3.266  acc@1: 0.267 acc@3: 0.400 acc@5: 0.467 acc@10: 0.667\n",
      "  best model so far\n",
      "TRAIN LOSS: 3.082  acc@1: 0.271 acc@3: 0.471 acc@5: 0.586 acc@10: 0.800\n",
      "VALID LOSS: 3.192  acc@1: 0.400 acc@3: 0.600 acc@5: 0.600 acc@10: 0.733\n",
      "  best model so far\n",
      "TRAIN LOSS: 2.991  acc@1: 0.400 acc@3: 0.586 acc@5: 0.643 acc@10: 0.829\n",
      "VALID LOSS: 3.334  acc@1: 0.533 acc@3: 0.600 acc@5: 0.600 acc@10: 0.733\n",
      "TRAIN LOSS: 2.990  acc@1: 0.457 acc@3: 0.586 acc@5: 0.657 acc@10: 0.829\n",
      "VALID LOSS: 3.133  acc@1: 0.600 acc@3: 0.733 acc@5: 0.733 acc@10: 0.733\n",
      "  best model so far\n",
      "TRAIN LOSS: 2.893  acc@1: 0.529 acc@3: 0.671 acc@5: 0.743 acc@10: 0.857\n",
      "VALID LOSS: 3.273  acc@1: 0.467 acc@3: 0.667 acc@5: 0.733 acc@10: 0.733\n",
      "TRAIN LOSS: 2.874  acc@1: 0.514 acc@3: 0.657 acc@5: 0.686 acc@10: 0.857\n",
      "VALID LOSS: 3.104  acc@1: 0.467 acc@3: 0.600 acc@5: 0.667 acc@10: 0.800\n",
      "  best model so far\n",
      "TRAIN LOSS: 2.761  acc@1: 0.529 acc@3: 0.700 acc@5: 0.829 acc@10: 0.900\n",
      "VALID LOSS: 3.158  acc@1: 0.467 acc@3: 0.667 acc@5: 0.733 acc@10: 0.800\n",
      "BEST VALID LOSS: 3.104  acc@1: 0.467 acc@3: 0.600 acc@5: 0.667 acc@10: 0.800\n"
     ]
    }
   ],
   "source": [
    "model = GNN()\n",
    "\n",
    "lr = LEARNING_RATE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lambda: lr)\n",
    "best_loss = np.inf\n",
    "for epoch in range(MAX_EPOCHS + 1):\n",
    "    train_loss, train_kacc = process(model, train_data, optimizer)\n",
    "    print(f\"TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], train_kacc)]))\n",
    "\n",
    "    valid_loss, valid_kacc = process(model, valid_data, None)\n",
    "    print(f\"VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], valid_kacc)]))\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        plateau_count = 0\n",
    "        best_loss = valid_loss\n",
    "        model.save_state('trained_params.pkl')\n",
    "        print(f\"  best model so far\")\n",
    "    else:\n",
    "        plateau_count += 1\n",
    "        if plateau_count % EARLY_STOPPING == 0:\n",
    "            print(f\"  {plateau_count} epochs without improvement, early stopping\")\n",
    "            break\n",
    "        if plateau_count % PATIENCE == 0:\n",
    "            lr *= 0.2\n",
    "            print(f\"  {plateau_count} epochs without improvement, decreasing learning rate to {lr}\")\n",
    "\n",
    "model.restore_state('trained_params.pkl')\n",
    "valid_loss, valid_kacc = process(model, valid_data, None)\n",
    "print(f\"BEST VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip([1, 3, 5, 10], valid_kacc)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
